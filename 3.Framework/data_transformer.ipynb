{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_arff(filepath):\n",
    "    data = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(data[0], dtype=np.float32)\n",
    "    df.to_csv(filepath.replace('.arff','.dat'),index=None, columns=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_arff('data/benchmark/realWorld/elecNormNew.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/benchmark/realWorld/elecNormNew.dat', header=None)\n",
    "# data['cls'] = data.iloc[:,-1].map(lambda x: 0 if x == \"b'UP'\" else 1)\n",
    "data.iloc[:,-1] = data.iloc[:,-1].map(lambda x: 0 if x == \"b'UP'\" else 1)\n",
    "\n",
    "df = pd.DataFrame(data.values)\n",
    "df.to_csv('data/benchmark/elecNormNew.csv', header=None, columns=None, index=None)\n",
    "# np.save('data/elecNormNew.npy', data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "\n",
    "def mm_norm(src, des, sep=',', header=False):\n",
    "    if header:\n",
    "        df = pd.read_csv(src, sep=sep)\n",
    "    else:\n",
    "        df = pd.read_csv(src,header=None, sep=sep)\n",
    "        \n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    df_mm = MinMaxScaler(feature_range=[0,2]).fit_transform(X)\n",
    "#     newdata = np.concatenate([df_mm, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    df_mm_norm = Normalizer().fit_transform(df_mm)\n",
    "    newdata = np.concatenate([df_mm_norm, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df.describe()\n",
    "    df.to_csv(des, index=None, columns=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "\n",
    "path = 'data/benchmark/realWorld/HAR'\n",
    "\n",
    "file_x = os.path.join(path,'X_train.txt')\n",
    "file_y = os.path.join(path,'y_train.txt')\n",
    "file_x_new = file_x.replace('.txt','.dat')\n",
    "des = 'data/data/har.csv'\n",
    "\n",
    "def har():\n",
    "    def trans(file_x):\n",
    "        with open(file_x) as fin:\n",
    "            with open(file_x_new,'w') as fout:\n",
    "                for line in fin.readlines():\n",
    "                    new_line = line.strip().replace('  ',' ').replace(' ',',')\n",
    "                    fout.write(f'{new_line}\\n')\n",
    "#     trans(file_x)\n",
    "\n",
    "    df_x = pd.read_csv(file_x_new, header=None)\n",
    "    df_y = pd.read_csv(file_y, header=None)\n",
    "    \n",
    "    data_x = df_x.values\n",
    "    data_y = df_y.values\n",
    "    \n",
    "    df_mm = MinMaxScaler(feature_range=[0,2]).fit_transform(data_x)\n",
    "#     newdata = np.concatenate([df_mm, data_y], axis=1)\n",
    "\n",
    "    df_mm_norm = Normalizer().fit_transform(df_mm)\n",
    "    newdata = np.concatenate([df_mm_norm, data_y], axis=1)\n",
    "\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df.to_csv(des, index=None, columns=None, header=None)\n",
    "\n",
    "har()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Forest Covertype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = 'data/dataset/covtype.data'\n",
    "des = 'data/data/covtype.csv'\n",
    "\n",
    "mm_norm(src, des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gas Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = 'data/dataset/gas-sensor.dat'\n",
    "des = 'data/data/gas-sensor.csv'\n",
    "\n",
    "mm_norm(src, des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# kddcup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '2' '12' '9' '1032' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '511' '511' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00'\n",
      " '0.00' '255' '255' '1.00' '0.00' '1.00' '0.00' '0.00' '0.00' '0.00'\n",
      " '0.00' '5']\n",
      "['0' '2' '12' '9' '1032' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '509' '509' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00'\n",
      " '0.00' '255' '255' '1.00' '0.00' '1.00' '0.00' '0.00' '0.00' '0.00'\n",
      " '0.00' '5']\n",
      "['0' '2' '12' '9' '1032' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '511' '511' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00'\n",
      " '0.00' '255' '255' '1.00' '0.00' '1.00' '0.00' '0.00' '0.00' '0.00'\n",
      " '0.00' '5']\n",
      "['0' '2' '12' '9' '520' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '478' '478' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00'\n",
      " '0.00' '255' '255' '1.00' '0.00' '1.00' '0.00' '0.00' '0.00' '0.00'\n",
      " '0.00' '5']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "global label_list\n",
    "label_list = []\n",
    "\n",
    "dat = 'data/dataset/kddcup.data_10_percent_corrected'\n",
    "src = 'data/dataset/kddcup.data_10_percent_corrected.dat'\n",
    "des = 'data/data/kddcup.data_10_percent_corrected.csv'\n",
    "\n",
    "def preprocess(src, des):\n",
    "    data_file = open(des, 'w', newline='')\n",
    "    with open(src) as data_source:\n",
    "        csv_reader = csv.reader(data_source)\n",
    "        csv_writer = csv.writer(data_file)\n",
    "        count = 0\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            temp_line = np.array(row)\n",
    "            temp_line[1] = handleProtocol(row)\n",
    "            temp_line[2] = handleService(row)\n",
    "            temp_line[3] = handleFlag(row)\n",
    "            temp_line[41] = handleLabel(row)\n",
    "            csv_writer.writerow(temp_line)\n",
    "            count += 1\n",
    "#             if count % 100000 == 0:\n",
    "#                 print(temp_line)\n",
    "    data_file.close()\n",
    "\n",
    "def handleProtocol(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中3种协议类型转换成数字标识的函数\n",
    "    \"\"\"\n",
    "    protocol_list = ['tcp', 'udp', 'icmp']\n",
    "    if input[1] in protocol_list:\n",
    "        return protocol_list.index(input[1])\n",
    "\n",
    "def handleService(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中70种网络服务类型转换成数字标识的函数\n",
    "    \"\"\"\n",
    "    service_list = ['aol', 'auth', 'bgp', 'courier', 'csnet_ns', 'ctf', 'daytime', 'discard', 'domain', 'domain_u',\n",
    "                    'echo', 'eco_i', 'ecr_i', 'efs', 'exec', 'finger', 'ftp', 'ftp_data', 'gopher', 'harvest',\n",
    "                    'hostnames',\n",
    "                    'http', 'http_2784', 'http_443', 'http_8001', 'imap4', 'IRC', 'iso_tsap', 'klogin', 'kshell',\n",
    "                    'ldap',\n",
    "                    'link', 'login', 'mtp', 'name', 'netbios_dgm', 'netbios_ns', 'netbios_ssn', 'netstat', 'nnsp',\n",
    "                    'nntp',\n",
    "                    'ntp_u', 'other', 'pm_dump', 'pop_2', 'pop_3', 'printer', 'private', 'red_i', 'remote_job', 'rje',\n",
    "                    'shell',\n",
    "                    'smtp', 'sql_net', 'ssh', 'sunrpc', 'supdup', 'systat', 'telnet', 'tftp_u', 'tim_i', 'time',\n",
    "                    'urh_i', 'urp_i',\n",
    "                    'uucp', 'uucp_path', 'vmnet', 'whois', 'X11', 'Z39_50']\n",
    "    if input[2] in service_list:\n",
    "        return service_list.index(input[2])\n",
    "\n",
    "def handleFlag(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中11种网络连接状态转换成数字标识的函数\n",
    "    \"\"\"\n",
    "    flag_list = ['OTH', 'REJ', 'RSTO', 'RSTOS0', 'RSTR', 'S0', 'S1', 'S2', 'S3', 'SF', 'SH']\n",
    "    if input[3] in flag_list:\n",
    "        return flag_list.index(input[3])\n",
    "\n",
    "def handleLabel(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中攻击类型转换成数字标识的函数(训练集中共出现了22个攻击类型，而剩下的17种只在测试集中出现)\n",
    "    \"\"\"\n",
    "    global label_list\n",
    "    if not input[41] in label_list:\n",
    "        label_list.append(input[41])\n",
    "    return label_list.index(input[41])\n",
    "\n",
    "\n",
    "def kddcup99():\n",
    "    preprocess(dat, src)\n",
    "    mm_norm(src, des)\n",
    "\n",
    "kddcup99()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = 'data/dataset/occupancy.txt'\n",
    "src = 'data/dataset/occupancy.dat'\n",
    "des = 'data/data/occupancy.txt'\n",
    "\n",
    "def occupancy():\n",
    "    def trans():\n",
    "        df = pd.read_csv(dat)\n",
    "        df = df.iloc[:,2:]\n",
    "        df.to_csv(src, index=None, columns=None, header=None)\n",
    "    \n",
    "    trans()\n",
    "    mm_norm(src, des)\n",
    "\n",
    "occupancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# poker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = 'data/dataset/poker-lsn.arff'\n",
    "src = 'data/dataset/poker-lsn.dat'\n",
    "des = 'data/data/poker-lsn.csv'\n",
    "\n",
    "# read_arff(dat)\n",
    "mm_norm(src, des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# sensorless-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = 'data/dataset/Sensorless_drive_diagnosis.txt'\n",
    "des = 'data/data/sensorless.csv'\n",
    "\n",
    "mm_norm(src, des, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# shuttle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = 'data/dataset/shuttle.trn'\n",
    "des = 'data/data/shuttle.csv'\n",
    "\n",
    "mm_norm(src, des, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   href   our  font  size color please align remove  face bgcolor  ...  \\\n",
      "0  b'0'  b'0'  b'1'  b'1'  b'1'   b'1'  b'0'   b'0'  b'1'    b'0'  ...   \n",
      "1  b'1'  b'1'  b'1'  b'1'  b'1'   b'1'  b'1'   b'0'  b'1'    b'0'  ...   \n",
      "2  b'0'  b'0'  b'0'  b'0'  b'0'   b'0'  b'0'   b'0'  b'0'    b'0'  ...   \n",
      "3  b'1'  b'1'  b'1'  b'1'  b'1'   b'1'  b'0'   b'0'  b'1'    b'0'  ...   \n",
      "4  b'0'  b'0'  b'0'  b'0'  b'0'   b'0'  b'0'   b'1'  b'0'    b'0'  ...   \n",
      "\n",
      "  receipt websites removeme 3dname 22http   555 promotions   101   was  \\\n",
      "0    b'0'     b'0'     b'0'   b'0'   b'0'  b'0'       b'0'  b'0'  b'0'   \n",
      "1    b'0'     b'0'     b'0'   b'1'   b'0'  b'0'       b'0'  b'0'  b'0'   \n",
      "2    b'0'     b'0'     b'0'   b'0'   b'0'  b'0'       b'0'  b'0'  b'0'   \n",
      "3    b'0'     b'0'     b'0'   b'0'   b'0'  b'0'       b'0'  b'0'  b'1'   \n",
      "4    b'0'     b'0'     b'0'   b'0'   b'0'  b'0'       b'0'  b'0'  b'0'   \n",
      "\n",
      "  spamorlegitimate  \n",
      "0          b'spam'  \n",
      "1          b'spam'  \n",
      "2          b'spam'  \n",
      "3          b'spam'  \n",
      "4          b'spam'  \n",
      "\n",
      "[5 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "dat = 'data/dataset/spam_data.arff'\n",
    "src = 'data/dataset/spam_data.dat'\n",
    "des = 'data/dataset/spam.csv'\n",
    "\n",
    "def spam():\n",
    "    def trans():\n",
    "        data = arff.loadarff(dat)\n",
    "        df = pd.DataFrame(data[0])\n",
    "        # # 样本\n",
    "        # 对标签进行处理\n",
    "        sample = df.values[:, :-1]\n",
    "        label = df.values[:, -1]\n",
    "        cla = []\n",
    "        dic = {\n",
    "            b'spam': 0,\n",
    "            b'legitimate': 1\n",
    "        }\n",
    "        for i in label:\n",
    "            cla.append(dic[i])\n",
    "        data = np.concatenate([sample, np.array(cla).reshape([-1, 1])], axis=1).astype(np.int8)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(src, index=None, columns=None, header=None)\n",
    "    trans()\n",
    "    mm_norm(src, des)\n",
    "spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spam():\n",
    "    from scipy.io import arff\n",
    "    import pandas as pd\n",
    "\n",
    "    filepath = 'data/spam/spam_data.arff'\n",
    "\n",
    "    data = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    # print(df.describe())\n",
    "    # print(df.head())\n",
    "    # # 样本\n",
    "    # 对标签进行处理\n",
    "    sample = df.values[:, :-1]\n",
    "    label = df.values[:, -1]\n",
    "    cla = []\n",
    "    dic = {\n",
    "        b'spam': 0,\n",
    "        b'legitimate': 1\n",
    "    }\n",
    "    for i in label:\n",
    "        cla.append(dic[i])\n",
    "    data = np.concatenate([sample, np.array(cla).reshape([-1, 1])], axis=1).astype(np.int8)\n",
    "    # np.save('data/spam/spam.array', data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('data/spam/spam.csv', index=None, columns=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  s1             r1             s2             r2  \\\n",
      "count  829201.000000  829201.000000  829201.000000  829201.000000   \n",
      "mean        2.376337       2.613158       2.506348       4.788464   \n",
      "std         1.113204       1.747380       1.116415       2.241039   \n",
      "min         1.000000       1.000000       1.000000       1.000000   \n",
      "25%         1.000000       1.000000       2.000000       3.000000   \n",
      "50%         2.000000       2.000000       3.000000       5.000000   \n",
      "75%         3.000000       4.000000       4.000000       6.000000   \n",
      "max         4.000000      12.000000       4.000000      13.000000   \n",
      "\n",
      "                  s3             r3             s4             r4  \\\n",
      "count  829201.000000  829201.000000  829201.000000  829201.000000   \n",
      "mean        2.499307       6.999647       2.493204       9.210980   \n",
      "std         1.117813       2.374738       1.116072       2.239432   \n",
      "min         1.000000       1.000000       1.000000       1.000000   \n",
      "25%         1.000000       5.000000       1.000000       8.000000   \n",
      "50%         3.000000       7.000000       2.000000       9.000000   \n",
      "75%         3.000000       9.000000       3.000000      11.000000   \n",
      "max         4.000000      13.000000       4.000000      13.000000   \n",
      "\n",
      "                  s5             r5          class  \n",
      "count  829201.000000  829201.000000  829201.000000  \n",
      "mean        2.625689      11.385760       0.617082  \n",
      "std         1.108804       1.742774       0.773174  \n",
      "min         1.000000       2.000000       0.000000  \n",
      "25%         2.000000      10.000000       0.000000  \n",
      "50%         3.000000      12.000000       0.000000  \n",
      "75%         4.000000      13.000000       1.000000  \n",
      "max         4.000000      13.000000       9.000000  \n",
      "    s1   r1   s2    r2   s3    r3   s4    r4   s5    r5  class\n",
      "0  1.0  1.0  1.0  10.0  1.0  11.0  1.0  12.0  2.0  13.0    4.0\n",
      "1  1.0  1.0  1.0  10.0  1.0  11.0  1.0  12.0  4.0  13.0    4.0\n",
      "2  1.0  1.0  1.0  10.0  1.0  11.0  2.0  11.0  1.0  12.0    1.0\n",
      "3  1.0  1.0  1.0  10.0  1.0  11.0  2.0  11.0  1.0  13.0    1.0\n",
      "4  1.0  1.0  1.0  10.0  1.0  11.0  2.0  11.0  2.0  12.0    1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filepath = 'data/poker-lsn.arff'\n",
    "\n",
    "data = arff.loadarff(filepath)\n",
    "df = pd.DataFrame(data[0], dtype=np.float32)\n",
    "print(df.describe())\n",
    "print(df.head())de\n",
    "# df = pd.DataFrame(df.values)\n",
    "# df.to_csv('data/poker-lsm.csv', index=None, columns=None, header=None)\n",
    "\n",
    "# # 样本\n",
    "# 对标签进行处理\n",
    "# sample = df.values[:, :-1]\n",
    "# label = df.values[:, -1]\n",
    "# cla = []\n",
    "# dic = {\n",
    "#     b'spam': 0,\n",
    "#     b'legitimate': 1\n",
    "# }\n",
    "# for i in label:\n",
    "#     cla.append(dic[i])\n",
    "# data = np.concatenate([sample, np.array(cla).reshape([-1, 1])], axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同一数据格式\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "task = 'occupancy'\n",
    "\n",
    "def mm_std(path, des, sep):\n",
    "    df = pd.read_csv(path,header=None, sep=sep)\n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    df_mm = MinMaxScaler().fit_transform(X)\n",
    "    df_mm = pd.DataFrame(df_mm)\n",
    "    print('df_mm:\\n', df_mm.describe())\n",
    "    newdata = np.concatenate([df_mm, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    # df_mm_std = StandardScaler().fit_transform(df_mm)\n",
    "    # df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    # print('df_mm_std:\\n', df_mm_std.describe())\n",
    "\n",
    "    # df_mm_std = Normalizer().fit_transform(df_mm)\n",
    "    # df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    # print('df_mm_std:\\n', df_mm_std.describe())\n",
    "    # newdata = np.concatenate([df_mm_std, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df.to_csv(des, index=None, columns=None, header=None)\n",
    "\n",
    "# iris数据\n",
    "def iris(path='data/iris/raw/iris.data'):\n",
    "    data = pd.read_csv(path, sep=',', header=None)\n",
    "    dic = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "    columns = data.columns\n",
    "    data[columns[-1]] = data[columns[-1]].map(dic)\n",
    "    data.to_csv(path.replace('.data', '.dat'), header=None, index=False, sep=' ')\n",
    "\n",
    "\n",
    "# iris(path)\n",
    "\n",
    "\n",
    "# weather 数据集\n",
    "def weather():\n",
    "    path = 'data/weather/weather.mat'\n",
    "    from scipy.io import loadmat\n",
    "    import numpy\n",
    "    annots = loadmat(path)\n",
    "    data = annots['weather']\n",
    "    numpy.savetxt(path.replace('.mat', '.tsv'), data, delimiter=',')\n",
    "\n",
    "    mm_std(path.replace('.mat','.tsv'),path.replace('.mat','.dsv'),sep=',')\n",
    "\n",
    "# shuttle\n",
    "def shuttle():\n",
    "    # path = 'data/shuttle/shuttle_Norm.trn'\n",
    "    # from scipy.io import loadmat\n",
    "    # import numpy\n",
    "    # annots = loadmat(path)\n",
    "    # data = annots['shuttle_Norm']\n",
    "    # numpy.savetxt(path.replace('.mat', '.csv'), data, delimiter=',')\n",
    "    src = 'data/shuttle/shuttle.trn'\n",
    "    des = 'data/shuttle/shuttle.csv'\n",
    "    mm_std(src, des, sep=' ')\n",
    "\n",
    "# spam 数据集\n",
    "def spam():\n",
    "    from scipy.io import arff\n",
    "    import pandas as pd\n",
    "\n",
    "    filepath = 'data/spam/spam_data.arff'\n",
    "\n",
    "    data = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    # print(df.describe())\n",
    "    # print(df.head())\n",
    "    # # 样本\n",
    "    # 对标签进行处理\n",
    "    sample = df.values[:, :-1]\n",
    "    label = df.values[:, -1]\n",
    "    cla = []\n",
    "    dic = {\n",
    "        b'spam': 0,\n",
    "        b'legitimate': 1\n",
    "    }\n",
    "    for i in label:\n",
    "        cla.append(dic[i])\n",
    "    data = np.concatenate([sample, np.array(cla).reshape([-1, 1])], axis=1).astype(np.int8)\n",
    "    # np.save('data/spam/spam.array', data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('data/spam/spam.csv', index=None, columns=None, header=None)\n",
    "\n",
    "# UCI\n",
    "def uci():\n",
    "    with open('data/UCI HAR Dataset/train/X_train.txt','r') as f:\n",
    "        length = []\n",
    "        for line in f.readlines():\n",
    "            l = line.strip().split(' ')\n",
    "            if len(l) not in length:\n",
    "                length.append(len(l))\n",
    "        print(length)\n",
    "\n",
    "    X = pd.read_csv('data/UCI HAR Dataset/train/X_train.txt', header=None, sep=' ')\n",
    "    print(X)\n",
    "    X = X.values\n",
    "    print(X)\n",
    "    y = pd.read_csv('data/UCI HAR Dataset/train/y_train.txt', header=None, sep=',').values\n",
    "    df_mm = MinMaxScaler().fit_transform(X)\n",
    "    df_mm = pd.DataFrame(df_mm * 100)\n",
    "    print('df_mm:\\n', df_mm.describe())\n",
    "    newdata = np.concatenate([df_mm, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df.to_csv(des, index=None, columns=None, header=None)\n",
    "\n",
    "# GSD\n",
    "def gsd():\n",
    "    # for i in range(1,11):\n",
    "    #     file = f'data/GSD/batch{i}.dat'\n",
    "    #     with open(file.replace('.dat','.csv'),'w') as fout:\n",
    "    #         with open(file) as f:\n",
    "    #             for line in f.readlines():\n",
    "    #                 line = line.strip().split(' ')\n",
    "    #                 t = []\n",
    "    #                 for j in line[1:]:\n",
    "    #                     t.append(j.split(':')[1]+',')\n",
    "    #                 s = ''.join(t)\n",
    "    #                 fout.write(s+line[0]+'\\n')\n",
    "    # for i in range(1,11):\n",
    "    #     file = f'data/GSD/batch{i}.csv'\n",
    "    #     mm_std(file,file.replace('.csv','.tsv'),',')\n",
    "    with open('data/GSD/gsd.csv', 'w') as fout:\n",
    "        lines = []\n",
    "        for i in range(1,11):\n",
    "            with open(f'data/GSD/batch{i}.tsv') as f:\n",
    "                lines.extend(f.readlines())\n",
    "        fout.writelines(lines)\n",
    "# kddcup99\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "global label_list\n",
    "label_list = []\n",
    "\n",
    "src_file = 'data/kddcup99/kddcup.data_10_percent_corrected'\n",
    "des_file = 'data/kddcup99/kddcup.data_10_percent_corrected.csv'\n",
    "\n",
    "def preprocess(src, des):\n",
    "    data_file = open(des, 'w', newline='')\n",
    "    with open(src) as data_source:\n",
    "        csv_reader = csv.reader(data_source)\n",
    "        csv_writer = csv.writer(data_file)\n",
    "        count = 0\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            temp_line = np.array(row)\n",
    "            temp_line[1] = handleProtocol(row)\n",
    "            temp_line[2] = handleService(row)\n",
    "            temp_line[3] = handleFlag(row)\n",
    "            temp_line[41] = handleLabel(row)\n",
    "            csv_writer.writerow(temp_line)\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                print(temp_line)\n",
    "    data_file.close()\n",
    "\n",
    "\n",
    "def handleProtocol(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中3种协议类型转换成数字标识的函数\n",
    "    \"\"\"\n",
    "    protocol_list = ['tcp', 'udp', 'icmp']\n",
    "    if input[1] in protocol_list:\n",
    "        return protocol_list.index(input[1])\n",
    "\n",
    "\n",
    "def handleService(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中70种网络服务类型转换成数字标识的函数\n",
    "    \"\"\"\n",
    "    service_list = ['aol', 'auth', 'bgp', 'courier', 'csnet_ns', 'ctf', 'daytime', 'discard', 'domain', 'domain_u',\n",
    "                    'echo', 'eco_i', 'ecr_i', 'efs', 'exec', 'finger', 'ftp', 'ftp_data', 'gopher', 'harvest',\n",
    "                    'hostnames',\n",
    "                    'http', 'http_2784', 'http_443', 'http_8001', 'imap4', 'IRC', 'iso_tsap', 'klogin', 'kshell',\n",
    "                    'ldap',\n",
    "                    'link', 'login', 'mtp', 'name', 'netbios_dgm', 'netbios_ns', 'netbios_ssn', 'netstat', 'nnsp',\n",
    "                    'nntp',\n",
    "                    'ntp_u', 'other', 'pm_dump', 'pop_2', 'pop_3', 'printer', 'private', 'red_i', 'remote_job', 'rje',\n",
    "                    'shell',\n",
    "                    'smtp', 'sql_net', 'ssh', 'sunrpc', 'supdup', 'systat', 'telnet', 'tftp_u', 'tim_i', 'time',\n",
    "                    'urh_i', 'urp_i',\n",
    "                    'uucp', 'uucp_path', 'vmnet', 'whois', 'X11', 'Z39_50']\n",
    "    if input[2] in service_list:\n",
    "        return service_list.index(input[2])\n",
    "\n",
    "\n",
    "def handleFlag(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中11种网络连接状态转换成数字标识的函数\n",
    "    \"\"\"\n",
    "    flag_list = ['OTH', 'REJ', 'RSTO', 'RSTOS0', 'RSTR', 'S0', 'S1', 'S2', 'S3', 'SF', 'SH']\n",
    "    if input[3] in flag_list:\n",
    "        return flag_list.index(input[3])\n",
    "\n",
    "\n",
    "def handleLabel(input):\n",
    "    \"\"\"\n",
    "    定义将源文件行中攻击类型转换成数字标识的函数(训练集中共出现了22个攻击类型，而剩下的17种只在测试集中出现)\n",
    "    \"\"\"\n",
    "    global label_list\n",
    "    if not input[41] in label_list:\n",
    "        label_list.append(input[41])\n",
    "    return label_list.index(input[41])\n",
    "\n",
    "\n",
    "def kddcup99():\n",
    "    preprocess(src_file, des_file)\n",
    "\n",
    "    df = pd.read_csv(des_file, header=None)\n",
    "\n",
    "    df_mm = MinMaxScaler().fit_transform(df)\n",
    "    df_mm = pd.DataFrame(df_mm)\n",
    "    print('df_mm:\\n', df_mm.describe())\n",
    "\n",
    "    df_mm_std = StandardScaler().fit_transform(df_mm.iloc[:, :-1])\n",
    "    df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    print('df_mm_std:\\n', df_mm_std.describe())\n",
    "\n",
    "    newdata = np.concatenate([df_mm_std, np.array(df.iloc[:, -1], dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df.to_csv('kddcup99.data.mm_std', index=None, columns=None, header=None)\n",
    "\n",
    "# sensor task\n",
    "def sensor():\n",
    "    src = 'data/sensor/sensor.txt'\n",
    "    des = 'data/sensor/sensor.csv'\n",
    "    mm_std(src, des, ' ')\n",
    "\n",
    "def covtyep():\n",
    "    src = 'data/covtype/covtype.data'\n",
    "    des = 'data/covtype/covtype.csv'\n",
    "    mm_std(src,des,sep=',')\n",
    "\n",
    "def poker():\n",
    "    src = 'data/poker-hand/poker-hand-training-true.data'\n",
    "    des = 'data/poker-hand/poker-hand.csv'\n",
    "    mm_std(src,des,sep=',')\n",
    "\n",
    "def occupancy():\n",
    "    src = 'data/occupancy_data/datatraining.txt'\n",
    "    des = 'data/occupancy_data/datatraining.csv'\n",
    "    mm_std(src, des, sep=',')\n",
    "\n",
    "def electricity():\n",
    "    src = 'data/ElectricityLoadDiagrams20112014/LD2011_2014.txt'\n",
    "    des = 'data/ElectricityLoadDiagrams20112014/LD2011_2014.csv'\n",
    "    data = pd.read_csv(src, sep=';')\n",
    "    df = data.iloc[:,1:]\n",
    "\n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    df_mm = MinMaxScaler().fit_transform(X)\n",
    "    df_mm = pd.DataFrame(df_mm)\n",
    "    print('df_mm:\\n', df_mm.describe())\n",
    "    newdata = np.concatenate([df_mm, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    # df_mm_std = StandardScaler().fit_transform(df_mm)\n",
    "    # df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    # print('df_mm_std:\\n', df_mm_std.describe())\n",
    "\n",
    "    # df_mm_std = Normalizer().fit_transform(df_mm)\n",
    "    # df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    # print('df_mm_std:\\n', df_mm_std.describe())\n",
    "    # newdata = np.concatenate([df_mm_std, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df = df.drop(columns=[2])\n",
    "    print(df)\n",
    "    df.to_csv(des, index=None, columns=None, header=None)\n",
    "\n",
    "def gas():\n",
    "    # file_names = os.listdir('data/gas-sensor')\n",
    "    # data = pd.read_csv(os.path.join(f'data/gas-sensor/{file_names[0]}'), sep=' ')\n",
    "    # data = data.values\n",
    "    # for file in file_names[1:]:\n",
    "    #     path = os.path.join('data/gas-sensor', file)\n",
    "    #     temp = pd.read_csv(path, sep=' ')\n",
    "    #     temp = temp.values\n",
    "    #     data = np.concatenate([data, temp])\n",
    "    # print(data.shape)\n",
    "    #\n",
    "    # data = pd.DataFrame(data)\n",
    "    # for i,col in enumerate(data.columns):\n",
    "    #    print(i, col)\n",
    "    # for i,col in enumerate(data.columns):\n",
    "    #     if i == 0:\n",
    "    #         continue\n",
    "    #     else:\n",
    "    #         data[col] = data[col].apply(lambda x:x.split(':')[1])\n",
    "    # data = np.concatenate([data.iloc[:,1:], np.array(data.iloc[:,0], dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "    # data = pd.DataFrame(data)\n",
    "    # data.to_csv('data/gas-sensor/data.dat', header=None, index=None)\n",
    "    src = 'data/gas-sensor/data.dat'\n",
    "    des = 'data/gas-sensor/data.csv'\n",
    "    mm_std(src,des,sep=',')\n",
    "\n",
    "def har():\n",
    "    src = 'data/HAR/train/X_train.txt'\n",
    "    with open('data/HAR/train/X_train.dat', 'w') as fout:\n",
    "        with open(src) as f:\n",
    "            for line in f.readlines():\n",
    "                newline = line.strip()\n",
    "                fout.write(newline)\n",
    "\n",
    "    des = 'data/HAR/x_train.csv'\n",
    "    X = pd.read_csv(src.replace('.txt','.dat'), sep=' ')\n",
    "    y = pd.read_csv(src.replace('X_train','y_train'), header=None).values\n",
    "    df_mm = MinMaxScaler().fit_transform(X)\n",
    "    df_mm = pd.DataFrame(df_mm)\n",
    "    print('df_mm:\\n', df_mm.describe())\n",
    "    newdata = np.concatenate([df_mm, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    # df_mm_std = StandardScaler().fit_transform(df_mm)\n",
    "    # df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    # print('df_mm_std:\\n', df_mm_std.describe())\n",
    "\n",
    "    # df_mm_std = Normalizer().fit_transform(df_mm)\n",
    "    # df_mm_std = pd.DataFrame(df_mm_std)\n",
    "    # print('df_mm_std:\\n', df_mm_std.describe())\n",
    "    # newdata = np.concatenate([df_mm_std, np.array(y, dtype=np.int8).reshape([-1, 1])], axis=1)\n",
    "\n",
    "    df = pd.DataFrame(newdata)\n",
    "    df.to_csv(des, index=None, columns=None, header=None)\n",
    "\n",
    "# *********************************************************************************************\n",
    "def dataPrepare():\n",
    "    # 读取正常数据，异常数据并合并\n",
    "\n",
    "    # 读取正常数据 使用数据 Danmini_Doorbell 物理设备。\n",
    "    data = pd.read_csv(\"data/BIot/Danmini_Doorbell/benign_traffic.csv\")\n",
    "    data['Class'] = 0\n",
    "\n",
    "    # 读取异常数据mirai_attacks\n",
    "    udp = pd.read_csv(\"data/BIot/Danmini_Doorbell/mirai_attacks/udp.csv\")\n",
    "    udp['Class'] = 1\n",
    "\n",
    "    ack = pd.read_csv(\"data/BIot/Danmini_Doorbell/mirai_attacks/ack.csv\")\n",
    "    ack['Class'] = 1\n",
    "\n",
    "    scan = pd.read_csv(\"data/BIot/Danmini_Doorbell/mirai_attacks/scan.csv\")\n",
    "    scan['Class'] = 1\n",
    "\n",
    "    syn = pd.read_csv(\"data/BIot/Danmini_Doorbell/mirai_attacks/syn.csv\")\n",
    "    syn['Class'] = 1\n",
    "\n",
    "    udpplain = pd.read_csv(\"data/BIot/Danmini_Doorbell/mirai_attacks/udpplain.csv\")\n",
    "    udpplain['Class'] = 1\n",
    "\n",
    "    # 读取异常数据gafgyt_attacks\n",
    "    g_combo = pd.read_csv(\"data/BIot/Danmini_Doorbell/gafgyt_attacks/combo.csv\")\n",
    "    g_combo['Class'] = 1\n",
    "\n",
    "    g_junk = pd.read_csv(\"data/BIot/Danmini_Doorbell/gafgyt_attacks/junk.csv\")\n",
    "    g_junk['Class'] = 1\n",
    "\n",
    "    g_scan = pd.read_csv(\"data/BIot/Danmini_Doorbell/gafgyt_attacks/scan.csv\")\n",
    "    g_scan['Class'] = 1\n",
    "\n",
    "    g_tcp = pd.read_csv(\"data/BIot/Danmini_Doorbell/gafgyt_attacks/tcp.csv\")\n",
    "    g_tcp['Class'] = 1\n",
    "\n",
    "    g_udp = pd.read_csv(\"data/BIot/Danmini_Doorbell/gafgyt_attacks/udp.csv\")\n",
    "    g_udp['Class'] = 1\n",
    "\n",
    "    frames = [data, udp, ack, scan, syn, udpplain\n",
    "        , g_combo, g_junk, g_scan, g_tcp, g_udp]\n",
    "\n",
    "    # 结果拼接\n",
    "    result = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    result.to_csv('danmini_doorbell.csv')\n",
    "    return result\n",
    "# *********************************************************************************************\n",
    "if task == 'kddcup99':\n",
    "    kddcup99()\n",
    "    src = 'data/kddcup99/kddcup.data_10_percent_corrected.csv'\n",
    "    des = 'data/kddcup99/kddcup.data.tsv'\n",
    "    mm_std(src,des,',')\n",
    "elif task == 'iris':\n",
    "    iris()\n",
    "elif task == 'weather':\n",
    "    weather()\n",
    "elif task == 'spam':\n",
    "    spam()\n",
    "elif task == 'sensor':\n",
    "    sensor()\n",
    "elif task == 'shuttle':\n",
    "    shuttle()\n",
    "elif task == 'UCI':\n",
    "    uci()\n",
    "elif task == 'GSD':\n",
    "    gsd()\n",
    "elif task == 'covtype':\n",
    "    covtyep()\n",
    "elif task == 'electricity':\n",
    "    electricity()\n",
    "elif task == 'gas':\n",
    "    gas()\n",
    "elif task == 'occupancy':\n",
    "    occupancy()\n",
    "elif task == 'poker':\n",
    "    poker()\n",
    "elif task == 'HAR':\n",
    "    har()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
