{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可靠性半监督数据流学习研究\n",
    "## 研究计划\n",
    "**StepOne**: 搭建完整个项目流程\n",
    "- [ ] 数据预处理：训练集和测试集，在此基础上，完成类别不平衡预处理\n",
    "- [ ] 项目结构梳理确认：各文件夹，文件等内容确定\n",
    "**Step Two**:完成loss修改\n",
    "- [ ] 度量学习\n",
    "- [ ] 对比学习\n",
    "**Step Three**：完成类别不平衡项目研究\n",
    "- [ ] 将微簇模型迁移到tensor上\n",
    "- [ ] 完成类别不平衡预测\n",
    "**Step Four**：实验验证即可视化工具\n",
    "**Step Five**：对比试验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "主要在utils中完成\n",
    "1. 从参数中读取需要处理的文件\n",
    "2. 将数据划分为训练集和测试集\n",
    "针对类别不平衡和类别平衡两种情况，首先确定各样本的数据比例，然后计算出需要筛选多少样本作为初始数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original class information: \n",
      "    classes  number\n",
      "0        0   31250\n",
      "1        1   31250\n",
      "2        2   31250\n",
      "3        3   31250\n",
      "imbalanced class information: \n",
      "    classes  number\n",
      "0        0    3125\n",
      "1        1    6732\n",
      "2        2   14504\n",
      "3        3   31250\n",
      "initial data information: \n",
      "    classes  number\n",
      "0        0     113\n",
      "1        1     243\n",
      "2        2     522\n",
      "3        3    1124\n",
      "stream data information: \n",
      "    classes  number\n",
      "0        0    3012\n",
      "1        1    6489\n",
      "2        2   13982\n",
      "3        3   30126\n",
      "masked initial data information:    classes  number\n",
      "0        0      40\n",
      "1        1      99\n",
      "2        2     218\n",
      "3        3     445\n",
      "4       -1    1200\n",
      "masked stream data information:    classes  number\n",
      "0        0    1200\n",
      "1        1    2606\n",
      "2        2    5521\n",
      "3        3   12117\n",
      "4       -1   32165\n",
      "save data to path: ./data/{init,eval}...\n"
     ]
    }
   ],
   "source": [
    "!python generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class information before sampling: \n",
      "    classes  number\n",
      "0        1   31250\n",
      "1        2   31250\n",
      "2        3   31250\n",
      "3        4   31250\n",
      "class information after sampling: \n",
      "    classes  number\n",
      "0        1     312\n",
      "1        2    1450\n",
      "2        3    6732\n",
      "3        4   31250\n",
      "   classes  number\n",
      "0        1       7\n",
      "1        2      36\n",
      "2        3     169\n",
      "3        4     786\n",
      "   classes  number\n",
      "0        1     305\n",
      "1        2    1414\n",
      "2        3    6563\n",
      "3        4   30464\n",
      "   classes  number\n",
      "0        1       5\n",
      "1        2      31\n",
      "2        3     134\n",
      "3        4     630\n",
      "   classes  number\n",
      "0        1     243\n",
      "1        2    1129\n",
      "2        3    5254\n",
      "3        4   24370\n",
      "   classes  number\n",
      "0        0       7\n",
      "1        1      36\n",
      "2        2     169\n",
      "3        3     786\n",
      "   classes  number\n",
      "0        0     305\n",
      "1        1    1414\n",
      "2        2    6563\n",
      "3        3   30464\n",
      "save data to path: ./data/{init,eval}/4CRE-V1.npy\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "opt = {'dataset':'4CRE-V1.txt',\n",
    "      'imb_ratio':100,\n",
    "       'imb_type':'long',\n",
    "      'label_ratio':20,\n",
    "      'num_max':1000,\n",
    "       'init_size':1000\n",
    "      }\n",
    "\n",
    "data_path = f\"data/benchmark/{opt['dataset']}\"\n",
    "\n",
    "def compute_class_info(labels):\n",
    "    \"\"\"\n",
    "    return the statistic information about class information\n",
    "    \"\"\"\n",
    "    classes = list(set(labels))\n",
    "    class_num = len(classes)\n",
    "    class_num_true_list = []\n",
    "    for c in classes:\n",
    "        class_num_true_list.append(len(np.where(labels==c)[0]))\n",
    "    class_info = np.array([classes, class_num_true_list], dtype=int).transpose()\n",
    "    class_info = pd.DataFrame(class_info, columns=['classes', 'number'])\n",
    "    class_info = class_info.sort_values(by='number')   \n",
    "    return class_info\n",
    "\n",
    "def compute_imb_class_info(class_info, init_size, imb_ratio, imb_type):\n",
    "    \"\"\"\n",
    "    description: according to the label and imbalance ratio to compute the number of each class in initial set and evaluate set\n",
    "    \n",
    "    \"\"\"\n",
    "    class_num_list = []\n",
    "    class_num = class_info.shape[0]\n",
    "    max_num = class_info.iloc[-1][-1]\n",
    "    gamma = imb_ratio\n",
    "    if imb_type == 'long':\n",
    "        mu = np.power(1/gamma, 1/(class_num - 1))\n",
    "        for i in range(class_num):\n",
    "            if i == (class_num - 1):\n",
    "                class_num_list.append(max_num / gamma)\n",
    "            else:\n",
    "                class_num_list.append(max_num * np.power(mu, i))\n",
    "    elif imb_type == 'step':\n",
    "        for i in range(class_num):\n",
    "            if i < int((class_num) / 2):\n",
    "                class_num_list.append(int(max_num))\n",
    "            else:\n",
    "                class_num_list.append(int(max_num / gamma))\n",
    "                \n",
    "    list.reverse(class_num_list)\n",
    "    for i in range(class_num):\n",
    "        if class_num_list[i] < class_info.iloc[i][-1]:\n",
    "            class_info.iloc[i][-1] = class_num_list[i]\n",
    "    return class_info\n",
    "\n",
    "def sample(labels, class_info, init_size):\n",
    "    \"\"\"\n",
    "    split the data into train and evaluate data,\n",
    "    \"\"\"\n",
    "    init_nums = np.array(class_info.iloc[:,-1], dtype=float) / np.sum(class_info.iloc[:,-1]) * init_size\n",
    "    init_nums = init_nums.astype(int)\n",
    "    \n",
    "    init_idxs = []\n",
    "    eval_idxs = []\n",
    "    for i in range(class_info.shape[0]):\n",
    "        label = class_info.iloc[i][0]\n",
    "        idxs = np.where(labels == label)[0] # return the index of data\n",
    "        np.random.shuffle(idxs)\n",
    "        idxs = idxs[:class_info.iloc[i,1]]\n",
    "        idxs = np.sort(idxs)\n",
    "        init_idxs.extend(idxs[:init_nums[i]])\n",
    "        eval_idxs.extend(idxs[init_nums[i]:])\n",
    "    return np.sort(init_idxs), np.sort(eval_idxs)\n",
    "\n",
    "def split_init_data(data_path, init_size, label_ratio, imb_ratio, imb_type='long'):\n",
    "    data = pd.read_csv(data_path, header=None, dtype=float)\n",
    "\n",
    "    labels = data.iloc[:,-1]\n",
    "    labels = np.array(data.iloc[:,-1], dtype=int)\n",
    "    \n",
    "    class_info = compute_class_info(labels)\n",
    "    print('class information before sampling: \\n', class_info)\n",
    "    \n",
    "    imb_class_info = compute_imb_class_info(class_info, init_size, imb_ratio, imb_type)\n",
    "    print('class information after sampling: \\n', imb_class_info)\n",
    "    \n",
    "    init_idxs, eval_idxs = sample(labels, class_info, init_size)\n",
    "    print(compute_class_info(labels[init_idxs]))\n",
    "    print(compute_class_info(labels[eval_idxs]))\n",
    "    \n",
    "    # random mask\n",
    "    init_mask = np.random.choice(init_idxs, size=int(init_size * (100.0 - label_ratio) / 100), replace=False)\n",
    "    eval_mask = np.random.choice(eval_idxs, size=int(len(eval_idxs) * (100.0 - label_ratio) / 100), replace=False)\n",
    "    print(compute_class_info(labels[init_mask]))\n",
    "    print(compute_class_info(labels[eval_mask]))\n",
    "    \n",
    "    # encode labels\n",
    "    cols = data.columns\n",
    "    classes = list(class_info.values[:,0])\n",
    "    encode_labels = data[cols[-1]].apply(lambda x:classes.index(int(x))).values.reshape((-1,1))\n",
    "    semi_labels = np.copy(encode_labels)\n",
    "    data = np.hstack([data.values[:,:-1], encode_labels, semi_labels])\n",
    "    \n",
    "    init_data = data[init_idxs]\n",
    "    eval_data = data[eval_idxs]\n",
    "    print(compute_class_info(init_data[:,-1]))\n",
    "    print(compute_class_info(eval_data[:,-1]))\n",
    "    \n",
    "    \n",
    "    file_name = opt['dataset'][:opt['dataset'].rindex('.')]+\".npy\"\n",
    "    print(r\"save data to path: ./data/{init,eval}/\",file_name,sep='')\n",
    "    np.save(f'./data/init/{file_name}', init_data, allow_pickle=False)\n",
    "    np.save(f'./data/eval/{file_name}', eval_data, allow_pickle=False)\n",
    "    \n",
    "split_init_data(data_path, opt['init_size'], opt['label_ratio'], opt['imb_ratio'], opt['imb_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 半监督表征学习\n",
    "## 度量学习\n",
    "度量学习通常是有监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {'dataset':'4CRE-V1',\n",
    "      'imb_ratio':100,\n",
    "       'imb_type':'long',\n",
    "      'label_ratio':20,\n",
    "      'num_max':1000,\n",
    "       'init_size':1000,\n",
    "       'weight_decay':0.0001,\n",
    "       'lr':0.0001,\n",
    "       'batch_size':32,\n",
    "       'num_epochs':4,\n",
    "       'train_eval_ratio':0.8,\n",
    "       'start_epoch':0,\n",
    "       'epochs':50,\n",
    "       'rate':0.1,\n",
    "       'cuda':False\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from pytorch_metric_learning.distances import LpDistance\n",
    "from pytorch_metric_learning.reducers import ThresholdReducer, AvgNonZeroReducer\n",
    "from pytorch_metric_learning.regularizers import LpRegularizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import config.config as cfg\n",
    "import utils.eval as eva\n",
    "from net.base import BaseModel\n",
    "from utils.data import DataSet\n",
    "from utils.plot import plot_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(2,5)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 28 18:14:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 515.57       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 35%   44C    P2    66W / 250W |    922MiB / 11264MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 35%   38C    P8    29W / 250W |     17MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  N/A |\n",
      "| 35%   42C    P8    23W / 250W |     17MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   31C    P8    11W / 300W |   4810MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     41814      C   python                            905MiB |\n",
      "|    3   N/A  N/A     47729      C   ...onda3/envs/131/bin/python     4793MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(layer_sizes, opt):\n",
    "    # load data\n",
    "    data_path = f\"./data/init/{opt.dataset}.npy\"\n",
    "\n",
    "    data = np.load(data_path, allow_pickle=False).astype('float32')\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Standardscaler or norminalizer\n",
    "    X = data[:, :-2]\n",
    "    Y = data[:, -2:].astype(np.int64)\n",
    "\n",
    "    scaler = Normalizer().fit(X)\n",
    "    # scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    plot_scatter(X[:, 0], X[:, 1], np.ones(X.shape[0]), Y[:, 0] * 52)\n",
    "\n",
    "    # data = torch.from_numpy(data)\n",
    "    train_num = int(X.shape[0] * opt.train_eval_ratio)\n",
    "    train_dataset = DataSet(X[:train_num], Y[:train_num])\n",
    "    eval_dataset = DataSet(X[train_num:], Y[train_num:])\n",
    "    print(len(train_dataset), len(eval_dataset))\n",
    "\n",
    "    # dataloader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # device : cuda or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # create model\n",
    "    models = BaseModel(layer_sizes)\n",
    "\n",
    "    models.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizers = torch.optim.Adam([{\"params\": models.trunk.parameters(), \"lr\": opt.lr},\n",
    "                                   {\"params\": models.embedder.parameters(), \"lr\": opt.lr},\n",
    "                                   {\"params\": models.classifier.parameters(), \"lr\": opt.lr}],\n",
    "                                  weight_decay=opt.weight_decay)\n",
    "\n",
    "    # classification loss\n",
    "    classification_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # metric loss & miner\n",
    "    distance = LpDistance(power=2)\n",
    "    miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "#     metric_loss = losses.MultiSimilarityLoss()\n",
    "    metric_loss = losses.TripletMarginLoss(distance=distance, reducer=AvgNonZeroReducer(), embedding_regularizer=LpRegularizer())\n",
    "    \n",
    "\n",
    "    criterions = [classification_loss, metric_loss]\n",
    "\n",
    "    # tensorboard\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(opt.start_epoch, opt.epochs):\n",
    "        print('Training in Epoch[{}]'.format(epoch))\n",
    "        adjust_learning_rate(optimizers, epoch, opt)\n",
    "\n",
    "        # train for one epoch\n",
    "        loss, acc = train(epoch, train_dataloader, models, criterions, miner, optimizers, device, opt)\n",
    "        writer.add_scalar(\"loss/train\", loss, epoch)\n",
    "        writer.add_scalar(\"acc/train\", loss, epoch)\n",
    "    nmi, recall, acc = validate(eval_dataloader, models, device, opt)\n",
    "    print(\n",
    "        'Recall@1, 2, 4, 8: {recall[0]:.3f}, {recall[1]:.3f}, {recall[2]:.3f}, {recall[3]:.3f}; NMI: {nmi:.3f}; accuracy: {acc:.3f} \\n'\n",
    "            .format(recall=recall, nmi=nmi, acc=acc))\n",
    "    save(models, scaler, opt)\n",
    "\n",
    "\n",
    "def train(epoch, train_loader, models, criterions, miner, optimizers, device, opt):\n",
    "    models.train()\n",
    "\n",
    "    # log item\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "    since = time.time()\n",
    "    for i, (inputs, target, _) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        trunk = models.trunk(inputs)\n",
    "        embedding = models.embedder(trunk)\n",
    "        output = models.classifier(embedding)\n",
    "\n",
    "        # loss\n",
    "        classification_loss = criterions[0](output, target)\n",
    "        pairs = miner(embedding, target)\n",
    "        metric_loss = criterions[1](embedding, target, pairs)\n",
    "        loss = classification_loss + 0.5 * metric_loss\n",
    "        # loss = metric_loss\n",
    "\n",
    "        # compute gradient and SGD step\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "\n",
    "        # predict result\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "    # compute the average loss and accuracy\n",
    "    epoch_loss = running_loss / 800\n",
    "    epoch_acc = float(running_corrects) / 800\n",
    "\n",
    "    stop = time.time()\n",
    "    print(\"Cost time: {time:.3f}s\".format(time=stop - since))\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, opt):\n",
    "    # decayed lr by 10 every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= opt.rate\n",
    "\n",
    "\n",
    "def validate(test_loader, models, device, opt):\n",
    "    models.eval()\n",
    "\n",
    "    acc = 0.0\n",
    "    testdata = torch.Tensor()\n",
    "    testlabel = torch.LongTensor()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target, _) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            trunk = models.trunk(inputs)\n",
    "            embedding = models.embedder(trunk)\n",
    "            output = models.classifier(embedding)\n",
    "\n",
    "            # predict result\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            acc += torch.sum(preds == target.data)\n",
    "\n",
    "            testdata = torch.cat((testdata, output.cpu()), 0)\n",
    "            testlabel = torch.cat((testlabel, target.cpu()))\n",
    "    acc = acc / 200.0\n",
    "    nmi, recall = eva.evaluation(testdata.numpy(), testlabel.numpy(), [1, 2, 4, 8])\n",
    "    return nmi, recall, acc\n",
    "\n",
    "\n",
    "def save(models, scaler, opt):\n",
    "    model_path = f\"./model/{opt.dataset}-model.pt\"\n",
    "    scaler_path = f\"./model/{opt.dataset}-scaler.pkl\"\n",
    "    torch.save(models, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "\n",
    "def transform(opt, init=True):\n",
    "    # load data\n",
    "    if init:\n",
    "        data_path = f\"./data/init/{opt.dataset}.npy\"\n",
    "        out_path = f\"./data/init/trans/{opt.dataset}.npy\"\n",
    "    else:\n",
    "        data_path = f\"./data/eval/{opt.dataset}.npy\"\n",
    "        out_path = f\"./data/eval/trans/{opt.dataset}.npy\"\n",
    "\n",
    "    data = np.load(data_path, allow_pickle=False).astype('float32')\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Standardscaler or norminalizer\n",
    "    X = data[:, :-2]\n",
    "    Y = data[:, -2:].astype(np.int64)\n",
    "\n",
    "    scaler_path = f\"./model/{opt.dataset}-scaler.pkl\"\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    # dataset\n",
    "    dataset = DataSet(X, Y)\n",
    "\n",
    "    # dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # device : cuda or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_path = f\"./model/{opt.dataset}-model.pt\"\n",
    "    models = torch.load(model_path)\n",
    "    models.to(device)\n",
    "    models.eval()\n",
    "\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target, mlabel) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            trunk = models.trunk(inputs)\n",
    "            embedding = models.embedder(trunk)\n",
    "\n",
    "            embeddings.append(\n",
    "                torch.hstack((embedding.cpu(), torch.unsqueeze(target.cpu(), -1), torch.unsqueeze(mlabel.cpu(), -1))))\n",
    "    embeddings = torch.vstack(embeddings)\n",
    "    embeddings = embeddings.numpy()\n",
    "\n",
    "    np.save(out_path, embeddings, allow_pickle=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = cfg.get_options()\n",
    "    #     transform(opt)\n",
    "    layer_sizes = [[2, 4], [4, 2], [2, 4]]\n",
    "    # layer_sizes = [[9,32,64],[64,16],[16,7]]\n",
    "    main(layer_sizes, opt)\n",
    "    print('finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比学习\n",
    "度量学习通常是在图像上进行，因为图像数据增强方法很多且合理。如何在一般数据上进行数据增广呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1; nl = 0; label = -1; ls = (5,); ss = (5,); t = 0; re = 1; ra = -1.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# 微簇，注意需要修改为向量模式\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MicroCluster:\n",
    "    def __init__(self, data, re=1, label=-1, radius=-1., lmbda=1e-4):\n",
    "        self.n = 1\n",
    "        self.nl = 0 if label == -1 else 1\n",
    "        self.ls = data\n",
    "        self.ss = np.square(data)\n",
    "        self.t = 0\n",
    "        self.re = re\n",
    "        self.label = label\n",
    "        self.radius = radius\n",
    "\n",
    "        self.lmbda = lmbda\n",
    "        self.epsilon = 0.00005\n",
    "        self.radius_factor = 1.1\n",
    "\n",
    "    def insert(self, data, labeled=False):\n",
    "        self.n += 1\n",
    "        self.nl += 1 if labeled else 0\n",
    "        self.ls += data\n",
    "        self.ss += np.square(data)\n",
    "        self.t = 0\n",
    "        # self.re = 1 if labeled else self.re       # 添加了这个地方:0901-18:34\n",
    "        self.radius = self.get_radius()\n",
    "\n",
    "    def update_reliability(self, probability, increase=True):\n",
    "        if increase:\n",
    "            self.re += max(1 - self.re, (1 - self.re) * math.pow(math.e, probability - 1))\n",
    "        else:\n",
    "            self.re -= (1 - self.re) * math.pow(math.e, probability)\n",
    "            # self.re -= 1 - math.pow(math.e, - probability)\n",
    "\n",
    "    def update(self):\n",
    "        self.t += 1\n",
    "        self.re = self.re * math.pow(math.e, - self.lmbda * self.epsilon * self.t)\n",
    "        return self.re\n",
    "\n",
    "    # 查\n",
    "    def get_deviation(self):\n",
    "        ls_mean = np.sum(np.square(self.ls / self.n))\n",
    "        ss_mean = np.sum(self.ss / self.n)\n",
    "        variance = ss_mean - ls_mean\n",
    "        variance = 1e-6 if variance < 1e-6 else variance\n",
    "        radius = np.sqrt(variance)\n",
    "        return radius\n",
    "\n",
    "    def get_center(self):\n",
    "        return self.ls / self.n\n",
    "\n",
    "    def get_radius(self):\n",
    "        if self.n <= 1:\n",
    "            return self.radius\n",
    "        return max(self.radius, self.get_deviation() * self.radius_factor)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"n = {self.n}; nl = {self.nl}; label = {self.label}; ls = {self.ls.shape}; ss = {self.ss.shape}; \" \\\n",
    "               f\"t = {self.t}; re = {self.re}; ra = {self.get_radius()}\\n \"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mc = MicroCluster(np.array([1, 2, 3, 4, 5]))\n",
    "    print(mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stream Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {'dataset':'4CRE-V1',\n",
    "      'imb_ratio':100,\n",
    "       'imb_type':'long',\n",
    "      'label_ratio':20,\n",
    "      'num_max':1000,\n",
    "       'init_size':1000,\n",
    "       'weight_decay':0.0001,\n",
    "       'lr':0.0001,\n",
    "       'batch_size':32,\n",
    "       'num_epochs':4,\n",
    "       'train_eval_ratio':0.8,\n",
    "       'start_epoch':0,\n",
    "       'epochs':50,\n",
    "       'rate':0.1,\n",
    "       'init_k_per_class':30,\n",
    "       'lambda':1e-4,\n",
    "       'cuda':True,\n",
    "       'knn':3,\n",
    "       'minRE':0.8,\n",
    "       'maxUMC':300,\n",
    "       'maxMC':1000,\n",
    "       'k':3\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(opt):\n",
    "    model_path = f\"./model/{opt['dataset']}-model.pt\"\n",
    "    scaler_path = f\"./model/{opt['dataset']}-scaler.pkl\"    \n",
    "    \n",
    "    model = torch.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model.eval()\n",
    "    return model, scaler\n",
    "    \n",
    "def initialization(opt):\n",
    "    global classes\n",
    "    global avg_radius\n",
    "    \n",
    "    # transform x from original space to embedding space\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model, scaler = load_model(opt)\n",
    "    model.to(device)\n",
    "    \n",
    "    path = f\"data/init/{opt['dataset']}.npy\"\n",
    "    init_data = np.load(path).astype(np.float32)\n",
    "    x = scaler.transform(init_data[:, :-2])\n",
    "    y = init_data[:,-2:].astype(np.int)\n",
    "    dataset = DataSet(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=opt['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    x_ebd = torch.Tensor()\n",
    "    for i, (inputs, tl, sl) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        trunk = model.trunk(inputs)\n",
    "        embedding = model.embedder(trunk)\n",
    "        \n",
    "        x_ebd = torch.cat((x_ebd, embedding.cpu()), 0)\n",
    "    \n",
    "    # initialize the micro-clusters\n",
    "    x = x_ebd.detach().numpy()\n",
    "    y = y[:,0]\n",
    "    classes = list(set(y))\n",
    "    \n",
    "    mcs_labeled = []\n",
    "    counter = {-1:0}\n",
    "    for cls in classes:\n",
    "        index = (y == cls)\n",
    "        data = x[index]\n",
    "        counter[cls] = 0\n",
    "\n",
    "        if len(data) > opt['init_k_per_class']:  # samples number is smaller than specific parameter\n",
    "            kmeans = KMeans(n_clusters=opt['init_k_per_class'])\n",
    "            kmeans.fit(data)\n",
    "            kmeans_labels = kmeans.labels_\n",
    "            for _cls in range(opt['init_k_per_class']):\n",
    "                _data_cls = data[kmeans_labels == _cls]\n",
    "                if len(_data_cls) == 0:\n",
    "                    continue\n",
    "                mc = MicroCluster(_data_cls[0], label=cls, lmbda=opt['lambda'])\n",
    "                for _d in _data_cls[1:]:\n",
    "                    mc.insert(_d, labeled=True)\n",
    "                mcs_labeled.append(mc)\n",
    "                counter[cls] += 1\n",
    "        else:\n",
    "            mc = MicroCluster(data[0], label=cls, lmbda=opt['lambda'])\n",
    "            for d in data[1:]:\n",
    "                mc.insert(d, labeled=True)\n",
    "            mcs_labeled.append(mc)\n",
    "            counter[cls] += 1\n",
    "\n",
    "    # self.avg_radius = np.max(np.array([mc.radius for mc in self.mcs_labeled if mc.n > 1]))\n",
    "    avg_radius = np.average(np.array([mc.radius for mc in mcs_labeled if mc.n > 1]))\n",
    "    logging.info(f'average radius : {avg_radius}')\n",
    "    for mc in mcs_labeled:\n",
    "        if mc.n <= 1:\n",
    "            mc.radius = avg_radius    \n",
    "    return mcs_labeled, counter, model, scaler\n",
    "    \n",
    "def ready_data(opt):\n",
    "    data_path = f\"data/eval/{opt['dataset']}.npy\"\n",
    "    \n",
    "    data = np.load(data_path)\n",
    "    labels = data[:,-2:].astype(np.int)\n",
    "    data = data[:,:-2].astype(np.float32)\n",
    "    \n",
    "    dataset = DataSet(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing the 0 instance\n",
      "computing the 1 instance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panliangxu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-916827b7f1d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-916827b7f1d1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-916827b7f1d1>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(dataloader, model, scaler, opt)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# compute distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemi_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-916827b7f1d1>\u001b[0m in \u001b[0;36mcal_distance\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    300\u001b[0m            [1.41421356]])\n\u001b[1;32m    301\u001b[0m     \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_norm_squared\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m         Y = check_array(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "lmcs = []\n",
    "umcs = []\n",
    "classes = []\n",
    "counter = {-1:0}\n",
    "avg_radius = 0.0\n",
    "create_num = 0\n",
    "\n",
    "def classify(dis, label_semi):\n",
    "    mcs = np.array([[mc.label, mc.re] for mc in lmcs])\n",
    "\n",
    "    # get topk of dis, class, reliability\n",
    "    if dis.shape[0] == 1:  # the number of mcs may decrease, smaller than args.k\n",
    "        topk_idx = np.array([0])\n",
    "        topk_dis = dis[topk_idx] + 1e-5\n",
    "    else:\n",
    "        k = min(opt['k'], len(lmcs) - 1)\n",
    "        topk_idx = np.argpartition(dis, k)[:k]\n",
    "        topk_dis = dis[topk_idx] + 1e-5\n",
    "        topk_dis /= np.min(topk_dis)\n",
    "    topk_cls = mcs[topk_idx, 0]\n",
    "    topk_res = mcs[topk_idx, 1]\n",
    "\n",
    "    # predict\n",
    "    ret_cls = np.zeros(len(classes))\n",
    "    ret_re = np.zeros(len(classes))\n",
    "    probabilities = softmax(topk_res / topk_dis)  ############# reliable distance\n",
    "    for i, cls in enumerate(topk_cls):\n",
    "        index = classes.index(cls)\n",
    "        ret_cls[index] += 1\n",
    "        ret_re[index] += probabilities[i]  # sum of topk's reliability of class index\n",
    "    label_pred = classes[np.argmax(ret_cls)]\n",
    "    re_pred = max(ret_re)\n",
    "\n",
    "    # update the reliability of topk if the true label is knownrt\n",
    "    if label_semi != -1:\n",
    "        correct = label_pred == label_semi\n",
    "        for i, cls in enumerate(topk_cls):\n",
    "            mc = lmcs[topk_idx[i]]\n",
    "            mc.update_reliability(probabilities[i], increase=correct)\n",
    "    return label_pred, re_pred\n",
    "\n",
    "def cal_distance(x):\n",
    "    lcs = np.array([mc.get_center() for mc in lmcs])\n",
    "    ucs = np.array([mc.get_center() for mc in umcs])\n",
    "    \n",
    "    if len(ucs) >= 1:\n",
    "        centers = np.vstack([lcs, ucs])\n",
    "    else:\n",
    "        centers = lcs\n",
    "    \n",
    "    dis = euclidean_distances(centers, x)\n",
    "    return dis.flatten()\n",
    "\n",
    "def insert_data(data, label_pred, label_semi, re, dis):\n",
    "    global create_num\n",
    "    known = False if label_semi == -1 else True\n",
    "\n",
    "    min_idx = np.argmin(dis)\n",
    "    if min_idx < len(lmcs):\n",
    "        nearest_mc = lmcs[min_idx]\n",
    "    else:\n",
    "        nearest_mc = umcs[min_idx - len(lmcs)]\n",
    "\n",
    "    if (dis[min_idx] < nearest_mc.radius) and (re >= opt['minRE']):\n",
    "        if known and (nearest_mc.label == label_semi or nearest_mc.label == -1):\n",
    "            nearest_mc.insert(data, labeled=known)\n",
    "            if nearest_mc.label == -1:\n",
    "                counter[label_semi] += 1\n",
    "                counter[-1] -= 1\n",
    "            nearest_mc.label = label_semi\n",
    "        elif not known and (nearest_mc.label == label_pred or nearest_mc.label == -1):\n",
    "            nearest_mc.insert(data, labeled=known)\n",
    "            if nearest_mc.label == -1:\n",
    "                counter[label_pred] += 1\n",
    "                counter[-1] -= 1\n",
    "            nearest_mc.label = label_pred\n",
    "        else:\n",
    "            if len(mcs_unlabeled) >= opt['maxUMC']:\n",
    "                drop(unlabeled=True)\n",
    "\n",
    "            if len(lmcs) >= opt['maxMC']:\n",
    "                drop()\n",
    "\n",
    "            re = 1 if known else re\n",
    "            label = label_semi if known else label_pred\n",
    "            mc = MicroCluster(data, re=re, label=label, radius=avg_radius, lmbda=opt['lambda'])\n",
    "            lmcs.append(mc)\n",
    "            # self.mcs.append(mc)\n",
    "\n",
    "            create_num += 1\n",
    "            counter[label] += 1\n",
    "    else:\n",
    "        if len(umcs) > opt['maxUMC']:\n",
    "            drop(unlabeled=True)\n",
    "\n",
    "        if len(lmcs) >= opt['maxMC']:\n",
    "            drop()\n",
    "\n",
    "        mc = MicroCluster(data, label=label_semi, radius=avg_radius, lmbda=opt['lambda'])\n",
    "        if label_semi == -1:\n",
    "            umcs.append(mc)\n",
    "        else:\n",
    "            lmcs.append(mc)\n",
    "\n",
    "        create_num += 1\n",
    "        counter[label_semi] += 1\n",
    "\n",
    "\n",
    "def drop(unlabeled=False):\n",
    "    def key(elem):\n",
    "        return elem.t\n",
    "\n",
    "    if unlabeled:\n",
    "        mcs = umcs\n",
    "    else:\n",
    "        mcs = lmcs\n",
    "\n",
    "    mcs.sort(key=key, reverse=False)  # 是否需要通过排序来解决，并且一次只删除一个，基本上每次都会删除\n",
    "    for mc in mcs[-opt['k']:]:\n",
    "        counter[mc.label] -= 1\n",
    "        if len(lmcs) == 0:\n",
    "            lmcs\n",
    "        mcs.remove(mc)\n",
    "\n",
    "def start(dataloader, model, scaler, opt):\n",
    "    device = torch.device('cuda' if opt['cuda'] & torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, (data, true_label, semi_label) in enumerate(dataloader):\n",
    "        print(f'computing the {i} instance')\n",
    "        data = data.to(device)\n",
    "        \n",
    "        x = model.trunk(data)\n",
    "        x = model.embedder(x).cpu().detach().numpy()\n",
    "        \n",
    "        # compute distance\n",
    "        dis = cal_distance(x)\n",
    "        pl, pr = classify(dis, semi_label.item())\n",
    "        \n",
    "        insert_data(x, pl, semi_label.item(), pr, dis)\n",
    "        if i == 1000:\n",
    "            break\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"total cost time: {cost:.3f}s\".format(cost=end-start))\n",
    "\n",
    "\n",
    "def main():\n",
    "    global lmcs\n",
    "    global umcs\n",
    "    global counter\n",
    "    \n",
    "    # initial\n",
    "    lmcs, cnt, model, scaler = initialization(opt)\n",
    "\n",
    "    dataloader = ready_data(opt)\n",
    "    \n",
    "    for cls in classes:\n",
    "        counter[cls] = 0\n",
    "    \n",
    "    # start\n",
    "    start(dataloader, model, scaler, opt)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/share/panliangxu/workspace/jupyter/OReSSL/3.Framework\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFy8plBnmXjhSd7glCRHAlzRbdFwO0JIonLvJNuOkCY6 2805420128@qq.com\n"
     ]
    }
   ],
   "source": [
    "!cat /home/panliangxu/.ssh/id_ed25519.pub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b04a640c22d8bcecd319ad1f5e650d2be704cc4469a07968663054afe80c9a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
