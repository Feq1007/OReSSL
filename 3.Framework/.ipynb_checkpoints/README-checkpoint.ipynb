{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可靠性半监督数据流学习研究\n",
    "## 研究计划\n",
    "**StepOne**: 搭建完整个项目流程\n",
    "- [ ] 数据预处理：训练集和测试集，在此基础上，完成类别不平衡预处理\n",
    "- [ ] 项目结构梳理确认：各文件夹，文件等内容确定\n",
    "**Step Two**:完成loss修改\n",
    "- [ ] 度量学习\n",
    "- [ ] 对比学习\n",
    "**Step Three**：完成类别不平衡项目研究\n",
    "- [ ] 将微簇模型迁移到tensor上\n",
    "- [ ] 完成类别不平衡预测\n",
    "**Step Four**：实验验证即可视化工具\n",
    "**Step Five**：对比试验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "主要在utils中完成\n",
    "1. 从参数中读取需要处理的文件\n",
    "2. 将数据划分为训练集和测试集\n",
    "针对类别不平衡和类别平衡两种情况，首先确定各样本的数据比例，然后计算出需要筛选多少样本作为初始数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class information before sampling: \n",
      "    classes  number\n",
      "0        1   31250\n",
      "1        2   31250\n",
      "2        3   31250\n",
      "3        4   31250\n",
      "class information after sampling: \n",
      "    classes  number\n",
      "0        1     312\n",
      "1        2    1450\n",
      "2        3    6732\n",
      "3        4   31250\n",
      "   classes  number\n",
      "0        1       7\n",
      "1        2      36\n",
      "2        3     169\n",
      "3        4     786\n",
      "   classes  number\n",
      "0        1     305\n",
      "1        2    1414\n",
      "2        3    6563\n",
      "3        4   30464\n",
      "   classes  number\n",
      "0        1       7\n",
      "1        2      25\n",
      "2        3     139\n",
      "3        4     629\n",
      "   classes  number\n",
      "0        1     245\n",
      "1        2    1142\n",
      "2        3    5188\n",
      "3        4   24421\n",
      "   classes  number\n",
      "0        2      11\n",
      "1        3      30\n",
      "2        4     157\n",
      "3       -1     800\n",
      "   classes  number\n",
      "0        1      60\n",
      "1        2     272\n",
      "2        3    1375\n",
      "3        4    6043\n",
      "4       -1   30996\n",
      "save data to path: ./data/{init,eval}...\n"
     ]
    }
   ],
   "source": [
    "!python generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class information before sampling: \n",
      "    classes  number\n",
      "0        1   31250\n",
      "1        2   31250\n",
      "2        3   31250\n",
      "3        4   31250\n",
      "class information after sampling: \n",
      "    classes  number\n",
      "0        1     312\n",
      "1        2    1450\n",
      "2        3    6732\n",
      "3        4   31250\n",
      "   classes  number\n",
      "0        1       7\n",
      "1        2      36\n",
      "2        3     169\n",
      "3        4     786\n",
      "   classes  number\n",
      "0        1     305\n",
      "1        2    1414\n",
      "2        3    6563\n",
      "3        4   30464\n",
      "   classes  number\n",
      "0        1       5\n",
      "1        2      31\n",
      "2        3     134\n",
      "3        4     630\n",
      "   classes  number\n",
      "0        1     243\n",
      "1        2    1129\n",
      "2        3    5254\n",
      "3        4   24370\n",
      "   classes  number\n",
      "0        0       7\n",
      "1        1      36\n",
      "2        2     169\n",
      "3        3     786\n",
      "   classes  number\n",
      "0        0     305\n",
      "1        1    1414\n",
      "2        2    6563\n",
      "3        3   30464\n",
      "save data to path: ./data/{init,eval}/4CRE-V1.npy\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "opt = {'dataset':'4CRE-V1.txt',\n",
    "      'imb_ratio':100,\n",
    "       'imb_type':'long',\n",
    "      'label_ratio':20,\n",
    "      'num_max':1000,\n",
    "       'init_size':1000\n",
    "      }\n",
    "\n",
    "data_path = f\"data/benchmark/{opt['dataset']}\"\n",
    "\n",
    "def compute_class_info(labels):\n",
    "    \"\"\"\n",
    "    return the statistic information about class information\n",
    "    \"\"\"\n",
    "    classes = list(set(labels))\n",
    "    class_num = len(classes)\n",
    "    class_num_true_list = []\n",
    "    for c in classes:\n",
    "        class_num_true_list.append(len(np.where(labels==c)[0]))\n",
    "    class_info = np.array([classes, class_num_true_list], dtype=int).transpose()\n",
    "    class_info = pd.DataFrame(class_info, columns=['classes', 'number'])\n",
    "    class_info = class_info.sort_values(by='number')   \n",
    "    return class_info\n",
    "\n",
    "def compute_imb_class_info(class_info, init_size, imb_ratio, imb_type):\n",
    "    \"\"\"\n",
    "    description: according to the label and imbalance ratio to compute the number of each class in initial set and evaluate set\n",
    "    \n",
    "    \"\"\"\n",
    "    class_num_list = []\n",
    "    class_num = class_info.shape[0]\n",
    "    max_num = class_info.iloc[-1][-1]\n",
    "    gamma = imb_ratio\n",
    "    if imb_type == 'long':\n",
    "        mu = np.power(1/gamma, 1/(class_num - 1))\n",
    "        for i in range(class_num):\n",
    "            if i == (class_num - 1):\n",
    "                class_num_list.append(max_num / gamma)\n",
    "            else:\n",
    "                class_num_list.append(max_num * np.power(mu, i))\n",
    "    elif imb_type == 'step':\n",
    "        for i in range(class_num):\n",
    "            if i < int((class_num) / 2):\n",
    "                class_num_list.append(int(max_num))\n",
    "            else:\n",
    "                class_num_list.append(int(max_num / gamma))\n",
    "                \n",
    "    list.reverse(class_num_list)\n",
    "    for i in range(class_num):\n",
    "        if class_num_list[i] < class_info.iloc[i][-1]:\n",
    "            class_info.iloc[i][-1] = class_num_list[i]\n",
    "    return class_info\n",
    "\n",
    "def sample(labels, class_info, init_size):\n",
    "    \"\"\"\n",
    "    split the data into train and evaluate data,\n",
    "    \"\"\"\n",
    "    init_nums = np.array(class_info.iloc[:,-1], dtype=float) / np.sum(class_info.iloc[:,-1]) * init_size\n",
    "    init_nums = init_nums.astype(int)\n",
    "    \n",
    "    init_idxs = []\n",
    "    eval_idxs = []\n",
    "    for i in range(class_info.shape[0]):\n",
    "        label = class_info.iloc[i][0]\n",
    "        idxs = np.where(labels == label)[0] # return the index of data\n",
    "        np.random.shuffle(idxs)\n",
    "        idxs = idxs[:class_info.iloc[i,1]]\n",
    "        idxs = np.sort(idxs)\n",
    "        init_idxs.extend(idxs[:init_nums[i]])\n",
    "        eval_idxs.extend(idxs[init_nums[i]:])\n",
    "    return np.sort(init_idxs), np.sort(eval_idxs)\n",
    "\n",
    "def split_init_data(data_path, init_size, label_ratio, imb_ratio, imb_type='long'):\n",
    "    data = pd.read_csv(data_path, header=None, dtype=float)\n",
    "\n",
    "    labels = data.iloc[:,-1]\n",
    "    labels = np.array(data.iloc[:,-1], dtype=int)\n",
    "    \n",
    "    class_info = compute_class_info(labels)\n",
    "    print('class information before sampling: \\n', class_info)\n",
    "    \n",
    "    imb_class_info = compute_imb_class_info(class_info, init_size, imb_ratio, imb_type)\n",
    "    print('class information after sampling: \\n', imb_class_info)\n",
    "    \n",
    "    init_idxs, eval_idxs = sample(labels, class_info, init_size)\n",
    "    print(compute_class_info(labels[init_idxs]))\n",
    "    print(compute_class_info(labels[eval_idxs]))\n",
    "    \n",
    "    # random mask\n",
    "    init_mask = np.random.choice(init_idxs, size=int(init_size * (100.0 - label_ratio) / 100), replace=False)\n",
    "    eval_mask = np.random.choice(eval_idxs, size=int(len(eval_idxs) * (100.0 - label_ratio) / 100), replace=False)\n",
    "    print(compute_class_info(labels[init_mask]))\n",
    "    print(compute_class_info(labels[eval_mask]))\n",
    "    \n",
    "    # encode labels\n",
    "    cols = data.columns\n",
    "    classes = list(class_info.values[:,0])\n",
    "    encode_labels = data[cols[-1]].apply(lambda x:classes.index(int(x))).values.reshape((-1,1))\n",
    "    semi_labels = np.copy(encode_labels)\n",
    "    data = np.hstack([data.values[:,:-1], encode_labels, semi_labels])\n",
    "    \n",
    "    init_data = data[init_idxs]\n",
    "    eval_data = data[eval_idxs]\n",
    "    print(compute_class_info(init_data[:,-1]))\n",
    "    print(compute_class_info(eval_data[:,-1]))\n",
    "    \n",
    "    \n",
    "    file_name = opt['dataset'][:opt['dataset'].rindex('.')]+\".npy\"\n",
    "    print(r\"save data to path: ./data/{init,eval}/\",file_name,sep='')\n",
    "    np.save(f'./data/init/{file_name}', init_data, allow_pickle=False)\n",
    "    np.save(f'./data/eval/{file_name}', eval_data, allow_pickle=False)\n",
    "    \n",
    "split_init_data(data_path, opt['init_size'], opt['label_ratio'], opt['imb_ratio'], opt['imb_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 半监督表征学习\n",
    "## 度量学习\n",
    "度量学习通常是有监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_metric_learning\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning import losses, miners, samplers, testers, trainers\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net.base import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mlp import MLP\n",
    "\n",
    "class Base(nn.Module):\n",
    "    def __init__(self, layer_sizes, final_relu=False):\n",
    "        super().__init__()\n",
    "        self.trunk = MLP(layer_sizes[0])\n",
    "        self.embedder = MLP(layer_sizes[1])\n",
    "        self.classifier = MLP(layer_sizes[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.trunk(x)\n",
    "        x = self.embedder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {'dataset':'4CRE-V1',\n",
    "      'imb_ratio':100,\n",
    "       'imb_type':'long',\n",
    "      'label_ratio':20,\n",
    "      'num_max':1000,\n",
    "       'init_size':1000,\n",
    "       'weight_decay':0.0001,\n",
    "       'lr':0.0001,\n",
    "       'batch_size':32,\n",
    "       'num_epochs':4,\n",
    "       'train_eval_ratio':0.8,\n",
    "       'start_epoch':0,\n",
    "       'epochs':50,\n",
    "       'rate':0.1,\n",
    "       'cuda':False\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils.eval as eva\n",
    "import joblib\n",
    "\n",
    "def train(epoch, train_loader, models, criterions, miner, optimizers, device, opt):\n",
    "#     for model in models:\n",
    "#         model.train()\n",
    "    models.train()\n",
    "    \n",
    "    # log item\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    \n",
    "    since = time.time()    \n",
    "    for i, (inputs, target, _) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # compute output\n",
    "#         trunk = models[0](inputs)\n",
    "#         embedding = models[1](trunk)\n",
    "#         output = models[2](embedding)\n",
    "        trunk = models.trunk(inputs)\n",
    "        embedding = models.embedder(trunk)\n",
    "        output = models.classifier(embedding)\n",
    "        \n",
    "        \n",
    "        # loss\n",
    "        classification_loss = criterions[0](output, target)\n",
    "        pairs = miner(embedding, target)\n",
    "        metric_loss = criterions[1](embedding, target, pairs)\n",
    "        loss = classification_loss + metric_loss\n",
    "        \n",
    "        # compute gradient and SGD step\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        # predict result\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "    # compute the average loss and accuracy\n",
    "    epoch_loss = running_loss / 800\n",
    "    epoch_acc = float(running_corrects) / 800\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    \n",
    "    writer.add_scalar('loss',epoch_loss, epoch)\n",
    "    writer.add_scalar('acc',epoch_acc, epoch)\n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, opt):\n",
    "    # decayed lr by 10 every 20 epochs\n",
    "    if (epoch+1)%20 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= opt['rate']\n",
    "            \n",
    "def validate(test_loader, models, device, opt):\n",
    "#     for model in models:\n",
    "#         model.eval()\n",
    "    models.eval()\n",
    "    \n",
    "    acc = 0.0\n",
    "    testdata = torch.Tensor()\n",
    "    testlabel = torch.LongTensor()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target, _) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "#             trunk = models[0](inputs)\n",
    "#             embedding = models[1](trunk)\n",
    "#             output = models[2](embedding)\n",
    "            trunk = models.trunk(inputs)\n",
    "            embedding = models.embedder(trunk)\n",
    "            output = models.classifier(embedding)\n",
    "            \n",
    "                    # predict result\n",
    "            _, preds = torch.max(output.data, 1)\n",
    "            acc += torch.sum(preds == target.data)\n",
    "            \n",
    "            testdata = torch.cat((testdata, output.cpu()), 0)\n",
    "            testlabel = torch.cat((testlabel, target.cpu()))\n",
    "    acc = acc / 200.0\n",
    "    nmi, recall = eva.evaluation(testdata.numpy(), testlabel.numpy(),[1,2,4,8])\n",
    "    return nmi, recall, acc\n",
    "\n",
    "def save(models, scaler, opt):\n",
    "    model_path = f\"./model/{opt['dataset']}-model.pt\"\n",
    "    scaler_path = f\"./model/{opt['dataset']}-scaler.pkl\"    \n",
    "    torch.save(models, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "# from tensorboardX import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('./logs/train_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.true_label = labels[:,0]\n",
    "        self.semi_label = labels[:,1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.true_label[index], self.semi_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798 200\n",
      "Training in Epoch[0]\n",
      "Train Loss: 1.4553 Acc: 0.1675\n",
      "Training in Epoch[1]\n",
      "Train Loss: 1.4445 Acc: 0.1675\n",
      "Training in Epoch[2]\n",
      "Train Loss: 1.4321 Acc: 0.0912\n",
      "Training in Epoch[3]\n",
      "Train Loss: 1.4250 Acc: 0.0400\n",
      "Training in Epoch[4]\n",
      "Train Loss: 1.4138 Acc: 0.0187\n",
      "Training in Epoch[5]\n",
      "Train Loss: 1.3995 Acc: 0.0063\n",
      "Training in Epoch[6]\n",
      "Train Loss: 1.3789 Acc: 0.0037\n",
      "Training in Epoch[7]\n",
      "Train Loss: 1.3784 Acc: 0.0063\n",
      "Training in Epoch[8]\n",
      "Train Loss: 1.3602 Acc: 0.0262\n",
      "Training in Epoch[9]\n",
      "Train Loss: 1.3521 Acc: 0.0587\n",
      "Training in Epoch[10]\n",
      "Train Loss: 1.3387 Acc: 0.1200\n",
      "Training in Epoch[11]\n",
      "Train Loss: 1.3270 Acc: 0.2350\n",
      "Training in Epoch[12]\n",
      "Train Loss: 1.3023 Acc: 0.5025\n",
      "Training in Epoch[13]\n",
      "Train Loss: 1.3079 Acc: 0.7875\n",
      "Training in Epoch[14]\n",
      "Train Loss: 1.2868 Acc: 0.7875\n",
      "Training in Epoch[15]\n",
      "Train Loss: 1.2810 Acc: 0.7875\n",
      "Training in Epoch[16]\n",
      "Train Loss: 1.2637 Acc: 0.7875\n",
      "Training in Epoch[17]\n",
      "Train Loss: 1.2536 Acc: 0.7875\n",
      "Training in Epoch[18]\n",
      "Train Loss: 1.2388 Acc: 0.7875\n",
      "Training in Epoch[19]\n",
      "Train Loss: 1.2301 Acc: 0.7875\n",
      "Training in Epoch[20]\n",
      "Train Loss: 1.2325 Acc: 0.7875\n",
      "Training in Epoch[21]\n",
      "Train Loss: 1.2327 Acc: 0.7875\n",
      "Training in Epoch[22]\n",
      "Train Loss: 1.2340 Acc: 0.7875\n",
      "Training in Epoch[23]\n",
      "Train Loss: 1.2245 Acc: 0.7875\n",
      "Training in Epoch[24]\n",
      "Train Loss: 1.2269 Acc: 0.7875\n",
      "Training in Epoch[25]\n",
      "Train Loss: 1.2191 Acc: 0.7875\n",
      "Training in Epoch[26]\n",
      "Train Loss: 1.2240 Acc: 0.7875\n",
      "Training in Epoch[27]\n",
      "Train Loss: 1.2238 Acc: 0.7875\n",
      "Training in Epoch[28]\n",
      "Train Loss: 1.2225 Acc: 0.7875\n",
      "Training in Epoch[29]\n",
      "Train Loss: 1.2216 Acc: 0.7875\n",
      "Training in Epoch[30]\n",
      "Train Loss: 1.2202 Acc: 0.7875\n",
      "Training in Epoch[31]\n",
      "Train Loss: 1.2191 Acc: 0.7875\n",
      "Training in Epoch[32]\n",
      "Train Loss: 1.2124 Acc: 0.7875\n",
      "Training in Epoch[33]\n",
      "Train Loss: 1.2155 Acc: 0.7875\n",
      "Training in Epoch[34]\n",
      "Train Loss: 1.2135 Acc: 0.7875\n",
      "Training in Epoch[35]\n",
      "Train Loss: 1.2145 Acc: 0.7875\n",
      "Training in Epoch[36]\n",
      "Train Loss: 1.2186 Acc: 0.7875\n",
      "Training in Epoch[37]\n",
      "Train Loss: 1.2102 Acc: 0.7875\n",
      "Training in Epoch[38]\n",
      "Train Loss: 1.2128 Acc: 0.7875\n",
      "Training in Epoch[39]\n",
      "Train Loss: 1.2040 Acc: 0.7875\n",
      "Training in Epoch[40]\n",
      "Train Loss: 1.2192 Acc: 0.7875\n",
      "Training in Epoch[41]\n",
      "Train Loss: 1.2034 Acc: 0.7875\n",
      "Training in Epoch[42]\n",
      "Train Loss: 1.1993 Acc: 0.7875\n",
      "Training in Epoch[43]\n",
      "Train Loss: 1.2045 Acc: 0.7875\n",
      "Training in Epoch[44]\n",
      "Train Loss: 1.2081 Acc: 0.7875\n",
      "Training in Epoch[45]\n",
      "Train Loss: 1.2061 Acc: 0.7875\n",
      "Training in Epoch[46]\n",
      "Train Loss: 1.2061 Acc: 0.7875\n",
      "Training in Epoch[47]\n",
      "Train Loss: 1.2135 Acc: 0.7875\n",
      "Training in Epoch[48]\n",
      "Train Loss: 1.2075 Acc: 0.7875\n",
      "Training in Epoch[49]\n",
      "Train Loss: 1.2048 Acc: 0.7875\n",
      "Recall@1, 2, 4, 8: 0.175, 0.175, 0.175, 0.175; NMI: 0.681; accuracy: 0.780 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "def main():\n",
    "    # device : cuda or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # create model\n",
    "    layer_sizes = [[2,4,8],[8,4],[4,4]]\n",
    "    models = BaseModel(layer_sizes)\n",
    "    \n",
    "    writer.add_graph(models, input_to_model=torch.rand(1,2))\n",
    "    \n",
    "    models.to(device)\n",
    "        \n",
    "    # optimizer\n",
    "    optimizers = torch.optim.Adam([{\"params\":models.trunk.parameters(),\"lr\":opt['lr']},\n",
    "                                  {\"params\":models.embedder.parameters(),\"lr\":opt['lr']},\n",
    "                                  {\"params\":models.classifier.parameters(),\"lr\":opt['lr']}], weight_decay=opt['weight_decay'])\n",
    "    \n",
    "    # classification loss\n",
    "    classification_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # metric loss & miner \n",
    "    metric_loss = losses.TripletMarginLoss(margin=0.1)\n",
    "    miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "    \n",
    "    criterions = [classification_loss, metric_loss]\n",
    "    \n",
    "    # load data\n",
    "    data_path = f\"./data/init/{opt['dataset']}.npy\"\n",
    "    \n",
    "    data = np.load(data_path, allow_pickle=False).astype('float32')\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Standardscaler or norminalizer\n",
    "    X = data[:,:-2]\n",
    "    Y = data[:,-2:].astype(int)\n",
    "\n",
    "    scaler = Normalizer().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "\n",
    "    # data = torch.from_numpy(data)\n",
    "    train_num = int(X.shape[0] * opt['train_eval_ratio'])\n",
    "    train_dataset = DataSet(X[:train_num], Y[:train_num])\n",
    "    eval_dataset = DataSet(X[train_num:], Y[train_num:])\n",
    "    print(len(train_dataset), len(eval_dataset))\n",
    "    \n",
    "    # dataloader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=opt['batch_size'], shuffle=True, num_workers=2)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=opt['batch_size'], shuffle=True, num_workers=2,)\n",
    "    \n",
    "    for epoch in range(opt['start_epoch'], opt['epochs']):\n",
    "        print('Training in Epoch[{}]'.format(epoch))\n",
    "        adjust_learning_rate(optimizers, epoch, opt)\n",
    "        \n",
    "        # train for one epoch\n",
    "        train(epoch, train_dataloader, models, criterions, miner, optimizers, device, opt)\n",
    "    nmi, recall, acc = validate(eval_dataloader, models, device, opt)\n",
    "    print('Recall@1, 2, 4, 8: {recall[0]:.3f}, {recall[1]:.3f}, {recall[2]:.3f}, {recall[3]:.3f}; NMI: {nmi:.3f}; accuracy: {acc:.3f} \\n'\n",
    "                  .format(recall=recall, nmi=nmi, acc=acc))  \n",
    "    save(models, scaler, opt)\n",
    "    writer.close()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比学习\n",
    "度量学习通常是在图像上进行，因为图像数据增强方法很多且合理。如何在一般数据上进行数据增广呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1; nl = 0; label = -1; ls = (5,); ss = (5,); t = 0; re = 1; ra = -1.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# 微簇，注意需要修改为向量模式\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MicroCluster:\n",
    "    def __init__(self, data, re=1, label=-1, radius=-1., lmbda=1e-4):\n",
    "        self.n = 1\n",
    "        self.nl = 0 if label == -1 else 1\n",
    "        self.ls = data\n",
    "        self.ss = np.square(data)\n",
    "        self.t = 0\n",
    "        self.re = re\n",
    "        self.label = label\n",
    "        self.radius = radius\n",
    "\n",
    "        self.lmbda = lmbda\n",
    "        self.epsilon = 0.00005\n",
    "        self.radius_factor = 1.1\n",
    "\n",
    "    def insert(self, data, labeled=False):\n",
    "        self.n += 1\n",
    "        self.nl += 1 if labeled else 0\n",
    "        self.ls += data\n",
    "        self.ss += np.square(data)\n",
    "        self.t = 0\n",
    "        # self.re = 1 if labeled else self.re       # 添加了这个地方:0901-18:34\n",
    "        self.radius = self.get_radius()\n",
    "\n",
    "    def update_reliability(self, probability, increase=True):\n",
    "        if increase:\n",
    "            self.re += max(1 - self.re, (1 - self.re) * math.pow(math.e, probability - 1))\n",
    "        else:\n",
    "            self.re -= (1 - self.re) * math.pow(math.e, probability)\n",
    "            # self.re -= 1 - math.pow(math.e, - probability)\n",
    "\n",
    "    def update(self):\n",
    "        self.t += 1\n",
    "        self.re = self.re * math.pow(math.e, - self.lmbda * self.epsilon * self.t)\n",
    "        return self.re\n",
    "\n",
    "    # 查\n",
    "    def get_deviation(self):\n",
    "        ls_mean = np.sum(np.square(self.ls / self.n))\n",
    "        ss_mean = np.sum(self.ss / self.n)\n",
    "        variance = ss_mean - ls_mean\n",
    "        variance = 1e-6 if variance < 1e-6 else variance\n",
    "        radius = np.sqrt(variance)\n",
    "        return radius\n",
    "\n",
    "    def get_center(self):\n",
    "        return self.ls / self.n\n",
    "\n",
    "    def get_radius(self):\n",
    "        if self.n <= 1:\n",
    "            return self.radius\n",
    "        return max(self.radius, self.get_deviation() * self.radius_factor)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"n = {self.n}; nl = {self.nl}; label = {self.label}; ls = {self.ls.shape}; ss = {self.ss.shape}; \" \\\n",
    "               f\"t = {self.t}; re = {self.re}; ra = {self.get_radius()}\\n \"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mc = MicroCluster(np.array([1, 2, 3, 4, 5]))\n",
    "    print(mc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stream Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {'dataset':'4CRE-V1',\n",
    "      'imb_ratio':100,\n",
    "       'imb_type':'long',\n",
    "      'label_ratio':20,\n",
    "      'num_max':1000,\n",
    "       'init_size':1000,\n",
    "       'weight_decay':0.0001,\n",
    "       'lr':0.0001,\n",
    "       'batch_size':32,\n",
    "       'num_epochs':4,\n",
    "       'train_eval_ratio':0.8,\n",
    "       'start_epoch':0,\n",
    "       'epochs':50,\n",
    "       'rate':0.1,\n",
    "       'init_k_per_class':30,\n",
    "       'lambda':1e-4,\n",
    "       'cuda':True,\n",
    "       'knn':3,\n",
    "       'minRE':0.8,\n",
    "       'maxUMC':300,\n",
    "       'maxMC':1000\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(opt):\n",
    "    model_path = f\"./model/{opt['dataset']}-model.pt\"\n",
    "    scaler_path = f\"./model/{opt['dataset']}-scaler.pkl\"    \n",
    "    \n",
    "    model = torch.load(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model.eval()\n",
    "    return model, scaler\n",
    "    \n",
    "def initialization(opt):\n",
    "    global classes\n",
    "    global avg_radius\n",
    "    \n",
    "    # transform x from original space to embedding space\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model, scaler = load_model(opt)\n",
    "    model.to(device)\n",
    "    \n",
    "    path = f\"data/init/{opt['dataset']}.npy\"\n",
    "    init_data = np.load(path).astype(np.float32)\n",
    "    x = scaler.transform(init_data[:, :-2])\n",
    "    y = init_data[:,-2:].astype(np.int)\n",
    "    dataset = DataSet(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=opt['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    x_ebd = torch.Tensor()\n",
    "    for i, (inputs, tl, sl) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        trunk = model.trunk(inputs)\n",
    "        embedding = model.embedder(trunk)\n",
    "        \n",
    "        x_ebd = torch.cat((x_ebd, embedding.cpu()), 0)\n",
    "    \n",
    "    # initialize the micro-clusters\n",
    "    x = x_ebd.detach().numpy()\n",
    "    y = y[:,0]\n",
    "    classes = list(set(y))\n",
    "    \n",
    "    mcs_labeled = []\n",
    "    counter = {-1:0}\n",
    "    for cls in classes:\n",
    "        index = (y == cls)\n",
    "        data = x[index]\n",
    "        counter[cls] = 0\n",
    "\n",
    "        if len(data) > opt['init_k_per_class']:  # samples number is smaller than specific parameter\n",
    "            kmeans = KMeans(n_clusters=opt['init_k_per_class'])\n",
    "            kmeans.fit(data)\n",
    "            kmeans_labels = kmeans.labels_\n",
    "            for _cls in range(opt['init_k_per_class']):\n",
    "                _data_cls = data[kmeans_labels == _cls]\n",
    "                if len(_data_cls) == 0:\n",
    "                    continue\n",
    "                mc = MicroCluster(_data_cls[0], label=cls, lmbda=opt['lambda'])\n",
    "                for _d in _data_cls[1:]:\n",
    "                    mc.insert(_d, labeled=True)\n",
    "                mcs_labeled.append(mc)\n",
    "                counter[cls] += 1\n",
    "        else:\n",
    "            mc = MicroCluster(data[0], label=cls, lmbda=opt['lambda'])\n",
    "            for d in data[1:]:\n",
    "                mc.insert(d, labeled=True)\n",
    "            mcs_labeled.append(mc)\n",
    "            counter[cls] += 1\n",
    "\n",
    "    # self.avg_radius = np.max(np.array([mc.radius for mc in self.mcs_labeled if mc.n > 1]))\n",
    "    avg_radius = np.average(np.array([mc.radius for mc in mcs_labeled if mc.n > 1]))\n",
    "    logging.info(f'average radius : {avg_radius}')\n",
    "    for mc in mcs_labeled:\n",
    "        if mc.n <= 1:\n",
    "            mc.radius = avg_radius    \n",
    "    return mcs_labeled, counter, model, scaler\n",
    "    \n",
    "def ready_data(opt):\n",
    "    data_path = f\"data/eval/{opt['dataset']}.npy\"\n",
    "    \n",
    "    data = np.load(data_path)\n",
    "    labels = data[:,-2:].astype(np.int)\n",
    "    data = data[:,:-2].astype(np.float32)\n",
    "    \n",
    "    dataset = DataSet(data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing the 0 instance\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "classify() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8943b37b2797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-8943b37b2797>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-8943b37b2797>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(dataloader, model, scaler, opt)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# compute distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemi_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0minsert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemi_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: classify() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "lmcs = []\n",
    "umcs = []\n",
    "classes = []\n",
    "counter = {-1:0}\n",
    "avg_radius = 0.0\n",
    "create_num = 0\n",
    "\n",
    "def classify(dis, label_semi):\n",
    "    mcs = np.array([[mc.label, mc.re] for mc in lmcs])\n",
    "\n",
    "    # get topk of dis, class, reliability\n",
    "    if dis.shape[0] == 1:  # the number of mcs may decrease, smaller than args.k\n",
    "        topk_idx = np.array([0])\n",
    "        topk_dis = dis[topk_idx] + 1e-5\n",
    "    else:\n",
    "        k = min(opt['k'], len(lmcs) - 1)\n",
    "        topk_idx = np.argpartition(dis, k)[:k]\n",
    "        topk_dis = dis[topk_idx] + 1e-5\n",
    "        topk_dis /= np.min(topk_dis)\n",
    "    topk_cls = mcs[topk_idx, 0]\n",
    "    topk_res = mcs[topk_idx, 1]\n",
    "\n",
    "    # predict\n",
    "    ret_cls = np.zeros(len(classes))\n",
    "    ret_re = np.zeros(len(classes))\n",
    "    probabilities = softmax(topk_res / topk_dis)  ############# reliable distance\n",
    "    for i, cls in enumerate(topk_cls):\n",
    "        index = classes.index(cls)\n",
    "        ret_cls[index] += 1\n",
    "        ret_re[index] += probabilities[i]  # sum of topk's reliability of class index\n",
    "    label_pred = classes[np.argmax(ret_cls)]\n",
    "    re_pred = max(ret_re)\n",
    "\n",
    "    # update the reliability of topk if the true label is knownrt\n",
    "    if label_semi != -1:\n",
    "        correct = label_pred == label_semi\n",
    "        for i, cls in enumerate(topk_cls):\n",
    "            mc = lmcs[topk_idx[i]]\n",
    "            mc.update_reliability(probabilities[i], increase=correct)\n",
    "    return label_pred, re_pred\n",
    "\n",
    "def cal_distance(x):\n",
    "    lcs = np.array([mc.get_center() for mc in lmcs])\n",
    "    ucs = np.array([mc.get_center() for mc in umcs])\n",
    "    \n",
    "    if len(ucs) >= 1:\n",
    "        centers = np.vstack([lcs, ucs])\n",
    "    else:\n",
    "        centers = lcs\n",
    "    \n",
    "    dis = euclidean_distances(centers, x)\n",
    "    return dis.flatten()\n",
    "\n",
    "def insert_data(data, label_pred, label_semi, re, dis):\n",
    "    known = False if label_semi == -1 else True\n",
    "\n",
    "    min_idx = np.argmin(dis)\n",
    "    if min_idx < len(lmcs):\n",
    "        nearest_mc = lmcs[min_idx]\n",
    "    else:\n",
    "        nearest_mc = umcs[min_idx - len(lmcs)]\n",
    "\n",
    "    if (dis[min_idx] < nearest_mc.radius) and (re >= opt['minRE']):\n",
    "        if known and (nearest_mc.label == label_semi or nearest_mc.label == -1):\n",
    "            nearest_mc.insert(data, labeled=known)\n",
    "            if nearest_mc.label == -1:\n",
    "                counter[label_semi] += 1\n",
    "                counter[-1] -= 1\n",
    "            nearest_mc.label = label_semi\n",
    "        elif not known and (nearest_mc.label == label_pred or nearest_mc.label == -1):\n",
    "            nearest_mc.insert(data, labeled=known)\n",
    "            if nearest_mc.label == -1:\n",
    "                counter[label_pred] += 1\n",
    "                counter[-1] -= 1\n",
    "            nearest_mc.label = label_pred\n",
    "        else:\n",
    "            if len(mcs_unlabeled) >= opt['maxUMC']:\n",
    "                drop(unlabeled=True)\n",
    "\n",
    "            if len(lmcs) >= opt['maxMC']:\n",
    "                drop()\n",
    "\n",
    "            re = 1 if known else re\n",
    "            label = label_semi if known else label_pred\n",
    "            mc = MicroCluster(data, re=re, label=label, radius=avg_radius, lmbda=opt['lambda'])\n",
    "            lmcs.append(mc)\n",
    "            # self.mcs.append(mc)\n",
    "\n",
    "            create_num += 1\n",
    "            counter[label] += 1\n",
    "    else:\n",
    "        if len(umcs) > opt['maxUMC']:\n",
    "            drop(unlabeled=True)\n",
    "\n",
    "        if len(lmcs) >= opt['maxMC']:\n",
    "            drop()\n",
    "\n",
    "        mc = MicroCluster(data, label=label_semi, radius=avg_radius, lmbda=opt['lambda'])\n",
    "        if label_semi == -1:\n",
    "            umcs.append(mc)\n",
    "        else:\n",
    "            lmcs.append(mc)\n",
    "\n",
    "        create_num += 1\n",
    "        counter[label_semi] += 1\n",
    "\n",
    "\n",
    "def drop(unlabeled=False):\n",
    "    def key(elem):\n",
    "        return elem.t\n",
    "\n",
    "    if unlabeled:\n",
    "        mcs = umcs\n",
    "    else:\n",
    "        mcs = lmcs\n",
    "\n",
    "    mcs.sort(key=key, reverse=False)  # 是否需要通过排序来解决，并且一次只删除一个，基本上每次都会删除\n",
    "    for mc in mcs[-opt['k']:]:\n",
    "        counter[mc.label] -= 1\n",
    "        if len(lmcs) == 0:\n",
    "            lmcs\n",
    "        mcs.remove(mc)\n",
    "\n",
    "def start(dataloader, model, scaler, opt):\n",
    "    device = torch.device('cuda' if opt['cuda'] & torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, (data, true_label, semi_label) in enumerate(dataloader):\n",
    "        print(f'computing the {i} instance')\n",
    "        data = data.to(device)\n",
    "        \n",
    "        x = model.trunk(data)\n",
    "        x = model.embedder(x).cpu().detach().numpy()\n",
    "        \n",
    "        # compute distance\n",
    "        dis = cal_distance(x)\n",
    "        pl, pr = classify(x, semi_label.item(), dis)\n",
    "        \n",
    "        insert_data(x, pl, semi_label.item(), pr, dis)\n",
    "        if i == 1000:\n",
    "            break\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"total cost time: {cost:.3f}s\".format(cost=end-start))\n",
    "\n",
    "\n",
    "def main():\n",
    "    global lmcs\n",
    "    global umcs\n",
    "    global counter\n",
    "    \n",
    "    # initial\n",
    "    lmcs, cnt, model, scaler = initialization(opt)\n",
    "\n",
    "    dataloader = ready_data(opt)\n",
    "    \n",
    "    for cls in classes:\n",
    "        counter[cls] = 0\n",
    "    \n",
    "    # start\n",
    "    start(dataloader, model, scaler, opt)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
